{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tests check it out\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "\n",
    "# in order to be reproducible\n",
    "rnd.seed(16180)\n",
    "def mySeed(seed=16180):\n",
    "    #tf.reset_default_graph()\n",
    "    #tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# dataset folder's name img-raw/ --> there are 8 subfolders\n",
    "dataset = \"img-raw/\"\n",
    "\n",
    "#page-book-80\n",
    "\n",
    "#utility to plot img\n",
    "def plot_color_image(image):\n",
    "    plt.imshow(image.astype(np.uint8),interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "#RGB img with 150x150px\n",
    "width = 150\n",
    "height = 150\n",
    "channels = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feed the net with img\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# change it in production\n",
    "PATH_TO_IMG = \"../img-raw\"\n",
    "\n",
    "# os commands\n",
    "#os.getcwd() #pwd\n",
    "imgFolder =os.listdir(PATH_TO_IMG) #folder list of a PATH\n",
    "imgFolder.sort() #sort the list, just in case Tumor and stroma logistic regression\n",
    "\n",
    "for cat in imgFolder:\n",
    "    #print(cat)\n",
    "    aux = os.path.join(PATH_TO_IMG, cat) #concatenate img folder + category subfolder\n",
    "    #print (aux)\n",
    "    #aux2 = os.listdir(os.path.join(PATH_TO_IMG, cat)) #list of all img of a folder without aux var\n",
    "    #print(type(aux2))\n",
    "    #print(os.listdir(aux)) #list of all img of a folder\n",
    "\n",
    "#csv vs listÂ¿?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# PATH_TO_IMG = \"../img-raw\"\n",
    "# list_images = [PATH_TO_IMG+f for f in os.listdir(PATH_TO_IMG) if re.search('tif|TIF', f)]\n",
    "\n",
    "# print(len(list_images))\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../img-raw\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# architecture relative paths for dataset folder change it accordingly\n",
    "INCEPTION_V3_PATH = \"../architecture/pretrained-model/inception_v3.ckpt\"\n",
    "INCEPTION_V4_PATH = \"../architecture/pretrained-model/inception_v4.ckpt\"\n",
    "VGG16_PATH = \"../architecture/pretrained-model/vgg_16.ckpt\"\n",
    "VGG19_PATH = \"../architecture/pretrained-model/vgg_19.ckpt\"\n",
    "RESNET_V1_50 = \"../architecture/pretrained-model/resnet_v1_50.ckpt\"\n",
    "RESNET_V1_152 = \"../architecture/pretrained-model/resnet_v1_152.ckpt\"\n",
    "\n",
    "# can I rename train.graph and eval.graph\n",
    "RESNET_V2_50 = \"../architecture/pretrained-model/\"\n",
    "RESNET_V2_152 = \"../architecture/pretrained-model/\"\n",
    "\n",
    "# to check\n",
    "ALEXNET_PATH = \"\"\n",
    "LENET_PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Once you have this program with you, use the following approach to use the model:\n",
    "\n",
    "from inception_resnet_v2 import inception_resnet_v2, inception_resnet_v2_arg_scope\n",
    "\n",
    "height = 299\n",
    "width = 299\n",
    "channels = 3\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, height, width, channels])\n",
    "with slim.arg_scope(inception_resnet_v2_arg_scope()):\n",
    "     logits, end_points = inception_resnet_v2(X, num_classes=1001,is_training=False)\n",
    "With this you have all the network in memory, Now you can initialize the network with checkpoint file(ckpt) by using tf.train.saver:\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "saver.restore(sess, \"/home/pramod/Downloads/inception_resnet_v2_2016_08_30.ckpt\")\n",
    "If you want to do bottle feature extraction, its simple like lets say you want to get features from last layer, then simply you have to declare predictions = end_points[\"Logits\"] If you want to get it for other intermediate layer, you can get those names from the above program inception_resnet_v2.py\n",
    "\n",
    "After that you can call: output =  sess.run(predictions, feed_dict={X:batch_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feed the net via tfrecords\n",
    "# http://www.machinelearninguru.com/deep_learning/tensorflow/basics/tfrecord/tfrecord.html\n",
    "# https://github.com/KaimingHe/deep-residual-networks --> graficos interesantes\n",
    "# http://ethereon.github.io/netscope/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the images are tagged by the name folder\n",
    "\n",
    "# https://stackoverflow.com/questions/6508576/what-is-the-difference-between-sys-and-os-sys\n",
    "#import os\n",
    "import sys\n",
    "\n",
    "# relative path (change it)!\n",
    "PATH_TO_IMG = \"../img-raw\"\n",
    "\n",
    "# like handson (use mine instead)?!\n",
    "imgCategories = sorted([category for category in os.listdir(PATH_TO_IMG)\n",
    "                  if os.path.isdir(os.path.join(PATH_TO_IMG, category))])\n",
    "\n",
    "#print(imgCategories)\n",
    "#type(imgCategories)\n",
    "\n",
    "\"\"\"\n",
    "getting all img's names for each category\n",
    "https://docs.python.org/2/library/collections.html#collections.defaultdict\n",
    "\n",
    "TO-DO --> make a function instead\n",
    "\"\"\"\n",
    "from collections import defaultdict\n",
    "\n",
    "#https://docs.python.org/3.3/library/collections.html#collections.defaultdict\n",
    "imgPaths = defaultdict(list)\n",
    "#type(imgPaths)\n",
    "\n",
    "# what's the best solution? get a dict or make a file instead\n",
    "\n",
    "for imgCategory in imgCategories:\n",
    "    imgFolder = os.path.join(PATH_TO_IMG, imgCategory)\n",
    "    for img in os.listdir(imgFolder):\n",
    "        if img.endswith(\".tif\"): #just in case there are other kind of files\n",
    "            imgPaths[imgCategory].append(os.path.join(imgFolder, img).replace(\"\\\\\",\"/\"))\n",
    "\n",
    "# sort imgPaths --> necesary?\n",
    "# for imgPath in imgPaths.values():\n",
    "#     imgPath.sort()  \n",
    "# imgPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feed the net via python\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.slim.nets import inception\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 299, 299, 3], name=\"X\")\n",
    "with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "    logits, end_points = inception.inception_v3(\n",
    "        X, num_classes=1001, is_training=False)\n",
    "predictions = end_points[\"Predictions\"]\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of img in the folder\n",
    "import os, os.path\n",
    "\n",
    "PATH_TO_IMG = \"../img-raw/\"\n",
    "\n",
    "# simple version for working with CWD\n",
    "print (len([name for name in os.listdir(PATH_TO_IMG) if os.path.isfile(name)]))\n",
    "\n",
    "# path joining version for other paths\n",
    "# DIR = '/tmp'\n",
    "# print len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "# images_dir =\"../img-raw/01_TUMOR\"\n",
    "images_dir =\"../img-raw\"\n",
    "# list_images = [images_dir+f for f in os.listdir(images_dir) if re.search('tif|TIF', f)]\n",
    "\n",
    "#len(list_images)\n",
    "#list_images\n",
    "\n",
    "# for counter, image in enumerate(list_images): # comienza en 0\n",
    "#     print(counter)\n",
    "#     print(image)\n",
    "#     print(re.split('_\\d+',image.split('/')[1])[0]) #img-raw\n",
    "#     print(re.split('_\\d+',image.split('/')[1])[0]) #img-raw\n",
    "#     print(re.split('_\\d+',image.split('/')[2])[0]) # nombre foto\n",
    "\n",
    "\n",
    "# myImage = '../img-raw/01_TUMOR/10009_CRC-Prim-HE-03_009.tif_Row_301_Col_151.tif'\n",
    "# print(re.split('_\\d+',myImage.split('/')[0])[0]) # parent ..\n",
    "# print(re.split('_\\d+',myImage.split('/')[1])[0]) #img-raw\n",
    "# print(re.split('_\\d+',myImage.split('/')[2])[0]) # nombre categoria\n",
    "# print(re.split('_\\d+',myImage.split('/')[3])[3]) # nombre foto\n",
    "\n",
    "#listado de las fotografias\n",
    "cos.listdir(images_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11\n",
    "htatps://www.kaggle.com/craigglastonbury/using-inceptionv3-features-svm-classifier/code\n",
    "    \n",
    "decodejpg \n",
    "https://stackoverflow.com/questions/34484148/feeding-image-data-in-tensorflow-for-transfer-learning\n",
    "\n",
    "# This is a script I applied early in toe competition ~ LB = 3. With bounding box regression  \n",
    "# (applying this to fishes rather than whole images achieves a much better score)\n",
    "# I used this script to learn about CNNs, feature extraction and using features learned by the InceptionV3 CNN\n",
    "# to perform classificaiton using a SVM architecture.\n",
    "# Inspired (adapted heavily) from: http://blog.christianperone.com/2015/08/convolutional-neural-networks-and-feature-extraction-with-python/\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.platform\n",
    "from tensorflow.python.platform import gfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn import svm\n",
    "\n",
    "model_dir = 'latest_submission/'\n",
    "# all training images\n",
    "images_dir = 'SVM_training_set/'\n",
    "list_images = [images_dir+f for f in os.listdir(images_dir) if re.search('jpg|JPG', f)]\n",
    "\n",
    "\n",
    "# setup tensorFlow graph initiation\n",
    "def create_graph():\n",
    "\twith gfile.FastGFile(os.path.join(model_dir, 'output.pb'), 'rb') as f:\n",
    "\t\tgraph_def = tf.GraphDef()\n",
    "\t\tgraph_def.ParseFromString(f.read())\n",
    "\t\t_ = tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "# extract all features from pool layer of InceptionV3\n",
    "def extract_features(list_images):\n",
    "\tnb_features = 2048\n",
    "\tfeatures = np.empty((len(list_images),nb_features))\n",
    "\tlabels = []\n",
    "\tcreate_graph()\n",
    "\twith tf.Session() as sess:\n",
    "\t\tnext_to_last_tensor = sess.graph.get_tensor_by_name('pool_3:0')\n",
    "\t\tfor ind, image in enumerate(list_images):\n",
    "\t\t\tprint('Processing %s...' % (image))\n",
    "\t\t\tif not gfile.Exists(image):\n",
    "\t\t\t\ttf.logging.fatal('File does not exist %s', image)\n",
    "\t\t\timage_data = gfile.FastGFile(image, 'rb').read()\n",
    "\t\t\tpredictions = sess.run(next_to_last_tensor,\n",
    "\t\t\t{'DecodeJpeg/contents:0': image_data})\n",
    "\t\t\tfeatures[ind,:] = np.squeeze(predictions)\n",
    "\t\t\tlabels.append(re.split('_\\d+',image.split('/')[1])[0])\n",
    "\t\treturn features, labels\n",
    "\n",
    "\n",
    "features,labels = extract_features(list_images)\n",
    "\n",
    "pickle.dump(features, open('features', 'wb'))\n",
    "pickle.dump(labels, open('labels', 'wb'))\n",
    "\n",
    "features = pickle.load(open('features'))\n",
    "labels = pickle.load(open('labels'))\n",
    "\n",
    "# run a 10-fold CV SVM using probabilistic outputs. \n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(features, labels, test_size=0.1, random_state=0)\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "\n",
    "# probabalistic SVM\n",
    "clf =  sklearn.calibration.CalibratedClassifierCV(svm)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict_proba(X_test)\n",
    "\n",
    "\n",
    "k_fold = KFold(len(labels),n_folds=10, shuffle=False, random_state=0)\n",
    "C_array=[0.001,0.01,0.1,1,10]\n",
    "C_scores=[]\n",
    "\n",
    "for k in C_array:\n",
    "\tclf = svm.SVC(kernel='linear', C=k)\n",
    "\tscores= cross_val_score(clf, features, labels, cv=k_fold, n_jobs=-1)\n",
    "\tC_scores.append(scores.mean())\n",
    "\tprint C_scores\n",
    "\n",
    "#C = 0.1 is best\n",
    "\n",
    "#clf = svm.LinearSVC(C=0.1)\n",
    "clf = svm.SVC(kernel='linear', C=0.1,probability=True)\n",
    "\n",
    "# final_model = clf.fit(features, labels)\n",
    "\n",
    "final_model = CalibratedClassifierCV(clf,cv=10,method='sigmoid')\n",
    "final_model = clf.fit(features, labels)\n",
    "\n",
    "\n",
    "test_dir='latest_submission/test_stg1/test_stg1/'\n",
    "list_images = [test_dir+f for f in os.listdir(test_dir) if re.search('jpg|JPG', f)]\n",
    "\n",
    "\n",
    "def extract_features(list_images):\n",
    "\tnb_features = 2048\n",
    "\tfeatures = np.empty((len(list_images),nb_features))\n",
    "\tcreate_graph()\n",
    "\twith tf.Session() as sess:\n",
    "\t\tnext_to_last_tensor = sess.graph.get_tensor_by_name('pool_3:0')\n",
    "\t\tfor ind, image in enumerate(list_images):\n",
    "\t\t\tprint('Processing %s...' % (image))\n",
    "\t\t\tif not gfile.Exists(image):\n",
    "\t\t\t\ttf.logging.fatal('File does not exist %s', image)\n",
    "\t\t\timage_data = gfile.FastGFile(image, 'rb').read()\n",
    "\t\t\tpredictions = sess.run(next_to_last_tensor,\n",
    "\t\t\t{'DecodeJpeg/contents:0': image_data})\n",
    "\t\t\tfeatures[ind,:] = np.squeeze(predictions)\n",
    "\t\treturn features\n",
    "\n",
    "\n",
    "features_test = extract_features(list_images)\n",
    "\n",
    "y_pred = final_model.predict_proba(features_test)\n",
    "#y_pred = final_model.predict(features_test)\n",
    "#y_pred = final_model.predict(features_test)\n",
    "\n",
    "\n",
    "image_id = [i.split('/')[3] for i in list_images]\n",
    "\n",
    "submit = open('submit.SVM.csv','w')\n",
    "submit.write('image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT\\n')\n",
    "\n",
    "for idx, id_n in enumerate(image_id):\n",
    "\tprobs=['%s' % p for p in list(y_pred[idx, :])]\n",
    "\tsubmit.write('%s,%s\\n' % (str(image_id[idx]),','.join(probs)))\n",
    "\n",
    "submit.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "img_path = '../img-raw/01_TUMOR/1A11_CRC-Prim-HE-07_022.tif_Row_601_Col_151.tif'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "features = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features.shape # 1 img + tensor / feature map 7x7 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://datascience.stackexchange.com/questions/16444/feature-extraction-for-a-pretrained-model-in-keras\n",
    "reshaped_features = features.reshape(1, 512*7*7)\n",
    "\n",
    "# type(reshaped_features)\n",
    "reshaped_features.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8eeb8a8ff083>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "model.layers\n",
    "\n",
    "img_path = '../img-raw/01_TUMOR/1A11_CRC-Prim-HE-07_022.tif_Row_601_Col_151.tif'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "features = model.predict(x)\n",
    "type(features)\n",
    "# print(\"hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../img-raw/01_TUMOR/1A11_CRC-Prim-HE-07_022.tif_Row_601_Col_151.tif'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of img\n",
    "img1 = '../img-raw/01_TUMOR/1A11_CRC-Prim-HE-07_022.tif_Row_601_Col_151.tif'\n",
    "img2 = '../img-raw/01_TUMOR/11D58_CRC-Prim-HE-09_011.tif_Row_151_Col_151.tif'\n",
    "\n",
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "devuelve el modelo keras con esa arquitectura devuelve un array\n",
    "1x7x7x512 --> buscamos la capa de pesos con 4096\n",
    "\n",
    "'''\n",
    "model_VGG16 = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# type(model_VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ejemplo comentado completamente\n",
    "\n",
    "'''\n",
    "en la clase https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py\n",
    "existe una funcion para procesar listas pero no incluye el formato .tif\n",
    "\n",
    "que sera mas optimo Â¿?\n",
    "    1.- preprocesar imagen a imagen --> pasar un tensor 1x224x224x3\n",
    "    2.- hacer una matriz de tensores (lista)\n",
    "\n",
    "\n",
    "#consola linux\n",
    "cat .keras/keras.json \n",
    "{\n",
    "    \"floatx\": \"float32\",\n",
    "    \"epsilon\": 1e-07,\n",
    "    \"backend\": \"tensorflow\",\n",
    "    \"image_data_format\": \"channels_last\"\n",
    "}\n",
    "'''\n",
    "\n",
    "img_path = '../img-raw/01_TUMOR/1A11_CRC-Prim-HE-07_022.tif_Row_601_Col_151.tif' # ruta a una imagen\n",
    "\n",
    "# http://pillow.readthedocs.io/en/3.4.x/reference/Image.html#PIL.Image.Image\n",
    "\n",
    "# load_img es objeto de keras --> target_size: img_height, img_width\n",
    "img = image.load_img(img_path, target_size=(224, 224)) \n",
    "\n",
    "# type(img) # PIL.Image.Image\n",
    "\n",
    "# cargado previamente, descarga el modelo BUSCAR donde se descarga\n",
    "model_VGG16 = VGG16(weights='imagenet', include_top=False) # devuelve el modelo keras con esa arquitectura\n",
    "\n",
    "# output\n",
    "# type(model_VGG16)\n",
    "# keras.engine.training.Model\n",
    "\n",
    "x = image.img_to_array(img) #se convierte a un array numpy la foto numpy.ndarray\n",
    "\n",
    "x.shape #(224, 224, 3)\n",
    "\n",
    "x = np.expand_dims(x, axis=0) # para que se convierta a un ndarray (1, 224, 224, 3) para luego convertirlo a un tensor\n",
    "\n",
    "'''\n",
    "https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py#L72\n",
    "preprocess_input(x, data_format=None, mode='caffe'):\n",
    "    # Arguments\n",
    "        x: input Numpy or symoblic tensor, 3D or 4D.\n",
    "        data_format: data format of the image tensor.\n",
    "        mode: One of \"caffe\", \"tf\".\n",
    "            - caffe: will convert the images from RGB to BGR,\n",
    "                then will zero-center each color channel with\n",
    "                respect to the ImageNet dataset,\n",
    "                without scaling.\n",
    "            - tf: will scale pixels between -1 and 1,\n",
    "                sample-wise.\n",
    "\n",
    "'''\n",
    "# ahora x es un tensor preprocesado type() --> numpy.ndarray\n",
    "# mode caffe\n",
    "x_caffe = preprocess_input(x)\n",
    "\n",
    "# mode tf revisar este punto\n",
    "# https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py#L15\n",
    "# x_tf = preprocess_input(x, data_format=None, mode='tf')\n",
    "\n",
    "x_caffe.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 7, 512)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features = model_VGG16.predict(x_caffe)\n",
    "\n",
    "# type(features) #numpy.ndarray como se usa caffe tienen como minimo el 0 \n",
    "\n",
    "features.shape #(1, 7, 7, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Required argument 'object' (pos 1) not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a6ba1b934820>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmatrizatributos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Required argument 'object' (pos 1) not found"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrizatributos = np.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing imagen 1..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jon/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"fc...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing imagen 2...\n"
     ]
    }
   ],
   "source": [
    "#input una lista de arrays\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# para sacar la capa fc2\n",
    "model_VGG16_lista = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "#list of img\n",
    "img1 = '../img-raw/01_TUMOR/1A11_CRC-Prim-HE-07_022.tif_Row_601_Col_151.tif'\n",
    "img2 = '../img-raw/01_TUMOR/11D58_CRC-Prim-HE-09_011.tif_Row_151_Col_151.tif'\n",
    "\n",
    "imgList = []\n",
    "# np.array\n",
    "deepFeatures = []\n",
    "\n",
    "imgList.append(img1)\n",
    "imgList.append(img2)\n",
    "\n",
    "# mejorar la obtencion de las 4096 deepfeatures\n",
    "\n",
    "for count, img in enumerate(imgList):\n",
    "    print('Processing imagen %d...' % (count+1))\n",
    "    aux = image.load_img(img, target_size=(224, 224))\n",
    "    x = image.img_to_array(aux)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "#     print(x.shape)\n",
    "    tensor = model_VGG16_lista.predict(x) # tensor 1x7x7x512 es obligatoriaÂ¿?\n",
    "#     print(tensor.shape)\n",
    "    imgFeatures = Model(input=model_VGG16_lista.input, output=model_VGG16_lista.get_layer('fc2').output)\n",
    "    fc2_features = imgFeatures.predict(x)\n",
    "    fc2_features = fc2_features.reshape((4096,1))\n",
    "    deepFeatures.append(fc2_features)\n",
    "\n",
    "    # da un aviso sobre la llamada de los inputs https://blog.keras.io/introducing-keras-2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2\n",
      "<class 'numpy.ndarray'>\n",
      "4096\n"
     ]
    }
   ],
   "source": [
    "print(type(deepFeatures))\n",
    "print(len(deepFeatures)) #numero de fotos\n",
    "print(type(deepFeatures[0]))\n",
    "print(len(deepFeatures[0])) #numero de features\n",
    "# ahora deepFeatures tiene una matriz con imgxdeepfeatures mejorar la estructura y guardarla en hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# guardar en un archivo hdf5 o con pickle dump\n",
    "# http://machinelearninguru.com/deep_learning/data_preparation/hdf5/hdf5.html\n",
    "# http://machinelearninguru.com/deep_learning/data_preparation/tfrecord/tfrecord.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/keras-team/keras/issues/4465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pca sobre columnas // GRP // deepfeatures selection\n",
    "# http://sebastianraschka.com/Articles/2014_pca_step_by_step.html\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)\n",
    "\n",
    "\n",
    "type(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
