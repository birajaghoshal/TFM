{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_labels & img_list - completed path to file\n",
    "PATH_TO_LABELS_FILE = \"../data/output_dataset/img_labels\"\n",
    "PATH_TO_PATHS_FILE = \"../data/output_dataset/img_list\"\n",
    "\n",
    "# output convnet files path\n",
    "PATH_TO_OUTPUT = \"../data/output_convnet\"\n",
    "\n",
    "img_labels = []\n",
    "img_list = []\n",
    "\n",
    "with open(PATH_TO_LABELS_FILE,'r') as f_img_labels:\n",
    "    for line in f_img_labels:\n",
    "        img_labels.append(line[:-1]) \n",
    "\n",
    "with open(PATH_TO_PATHS_FILE,'r') as f_img_list:\n",
    "    for line in f_img_list:\n",
    "        img_list.append(\"../data/\"+line[:-1]) # para eliminar el retorno de carro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(150, 150, 3))\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el top de la arquitectura original no es este\n",
    "# https://github.com/fchollet/deep-learning-models/blob/master/inception_v3.py#L342\n",
    "x = base_model.output\n",
    "\n",
    "x = GlobalAveragePooling2D()(x) # cambiar?\n",
    "\n",
    "x = Dense(4096, activation= None, name=\"fc1\")(x) # cambiar a 1024 si no funciona\n",
    "\n",
    "predictions = Dense(8, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "df_model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, layer in enumerate(df_model.layers):\n",
    "#     print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from sys import getsizeof\n",
    "\n",
    "# deep_features=4096\n",
    "# dfmap = np.empty((len(img_list),deep_features))\n",
    "# dfmap_1 = np.empty((len(img_list)//2,deep_features))\n",
    "# dfmap_2 = np.empty((len(img_list)//2,deep_features))\n",
    "\n",
    "# dfmap = np.empty((20,deep_features))\n",
    "# dfmap_1 = np.empty((10,deep_features))\n",
    "# dfmap_2 = np.empty((10,deep_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfmap_calc(img_list):\n",
    "    \n",
    "    deep_features=4096\n",
    "    dfmap = np.empty((len(img_list),deep_features))\n",
    "    \n",
    "    for index, img_path in enumerate(img_list):\n",
    "    \n",
    "#     index += 1\n",
    "    \n",
    "        if not index % 100:\n",
    "            print(\"image {0:d} processed\".format(index))\n",
    "\n",
    "        inception_layer = Model(input=df_model.input, output=df_model.get_layer('fc1').output)\n",
    "\n",
    "        # pre procesado de imagen\n",
    "        img = image.load_img(img_path, target_size=(299, 299), interpolation='lanczos')\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "\n",
    "\n",
    "\n",
    "        # model predict\n",
    "        x = inception_layer.predict(x)\n",
    "\n",
    "        dfmap[index,:] = np.squeeze(x, axis=0)\n",
    "\n",
    "    return dfmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# ti_dfmap = time.time()\n",
    "\n",
    "# dfmap_1 = dfmap_calc(img_list[:2500])\n",
    "\n",
    "# tf_dfmap = time.time()    \n",
    "# tt_dfmap = tf_dfmap - ti_dfmap\n",
    "# print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_dfmap)))\n",
    "# pickle.dump(dfmap_1, open('../data/output_convnet/inception_dfmap_1_pickle', 'wb'))\n",
    "\n",
    "# dfmap_1.shape\n",
    "\n",
    "# ti_dfmap = time.time()\n",
    "\n",
    "# dfmap_2 = dfmap_calc(img_list[2500:])\n",
    "\n",
    "# tf_dfmap = time.time()    \n",
    "# tt_dfmap = tf_dfmap - ti_dfmap\n",
    "# print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_dfmap)))\n",
    "# pickle.dump(dfmap_2, open('../data/output_convnet/inception_dfmap_2_pickle', 'wb'))\n",
    "\n",
    "# dfmap_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done in two runs (given the memmory overload)\n",
    "import os\n",
    "\n",
    "PATH_TO_DF = \"../data/output_convnet\"\n",
    "DF_MAP_1 = \"inception_dfmap_1_pickle\"\n",
    "DF_MAP_2 = \"inception_dfmap_2_pickle\"\n",
    "\n",
    "\n",
    "features_1 = pickle.load(open(os.path.join(PATH_TO_DF,DF_MAP_1),'rb'))\n",
    "features_2 = pickle.load(open(os.path.join(PATH_TO_DF,DF_MAP_2),'rb'))\n",
    "\n",
    "dfmap = np.append(features_1,features_2, axis=0)\n",
    "type(dfmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfmap = np.append(dfmap_1,dfmap_2, axis=0)\n",
    "\n",
    "# print(\"Tamaño en memoria de la matriz de características profundas: {0:.2f}Mb\".format(getsizeof(dfmap)/float(1<<20)))\n",
    "# pickle.dump(dfmap, open('../data/output_convnet/inception_dfmap_pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # init time\n",
    "# ti_dfmap = time.time()\n",
    "\n",
    "# for index, img_path in enumerate(img_list):\n",
    "    \n",
    "# #     index += 1\n",
    "    \n",
    "#     if not index % 100:\n",
    "#         print(\"image {0:d} processed\".format(index))\n",
    "    \n",
    "#     inception_layer = Model(input=df_model.input, output=df_model.get_layer('fc1').output)\n",
    "\n",
    "#     # pre procesado de imagen\n",
    "#     img = image.load_img(img_path, target_size=(299, 299), interpolation='lanczos')\n",
    "#     x = image.img_to_array(img)\n",
    "#     x = np.expand_dims(x, axis=0)\n",
    "#     x = preprocess_input(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # model predict\n",
    "#     x = inception_layer.predict(x)\n",
    "    \n",
    "#     dfmap[index,:] = np.squeeze(x, axis=0)\n",
    "\n",
    "# # stop time\n",
    "# tf_dfmap = time.time()    \n",
    "# tt_dfmap = tf_dfmap - ti_dfmap\n",
    "# print()\n",
    "# print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_dfmap)))\n",
    "# print()\n",
    "# print(\"Tamaño en memoria de la matriz de características profundas: {0:.2f}Mb\".format(getsizeof(dfmap)/float(1<<20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # init time\n",
    "# ti_dfmap = time.time()\n",
    "\n",
    "# for index, img_path in enumerate(img_list[:10]):\n",
    "    \n",
    "# #     index += 1\n",
    "    \n",
    "# #     if not index % 100:\n",
    "# #         print(\"image {0:d} processed\".format(index))\n",
    "    \n",
    "#     inception_layer = Model(input=df_model.input, output=df_model.get_layer('fc1').output)\n",
    "\n",
    "#     # pre procesado de imagen\n",
    "#     img = image.load_img(img_path, target_size=(299, 299), interpolation='lanczos')\n",
    "#     x = image.img_to_array(img)\n",
    "#     x = np.expand_dims(x, axis=0)\n",
    "#     x = preprocess_input(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # model predict\n",
    "#     x = inception_layer.predict(x)\n",
    "    \n",
    "#     dfmap_1[index,:] = np.squeeze(x, axis=0)\n",
    "\n",
    "# # stop time\n",
    "# tf_dfmap = time.time()    \n",
    "# tt_dfmap = tf_dfmap - ti_dfmap\n",
    "# print()\n",
    "# print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_dfmap)))\n",
    "# print()\n",
    "# print(\"Tamaño en memoria de la matriz de características profundas: {0:.2f}Mb\".format(getsizeof(dfmap_1)/float(1<<20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # init time\n",
    "# ti_dfmap = time.time()\n",
    "\n",
    "# for index, img_path in enumerate(img_list[10:20]):\n",
    "    \n",
    "# #     index += 1\n",
    "    \n",
    "# #     if not index % 100:\n",
    "# #         print(\"image {0:d} processed\".format(index))\n",
    "    \n",
    "#     inception_layer = Model(input=df_model.input, output=df_model.get_layer('fc1').output)\n",
    "\n",
    "#     # pre procesado de imagen\n",
    "#     img = image.load_img(img_path, target_size=(299, 299), interpolation='lanczos')\n",
    "#     x = image.img_to_array(img)\n",
    "#     x = np.expand_dims(x, axis=0)\n",
    "#     x = preprocess_input(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # model predict\n",
    "#     x = inception_layer.predict(x)\n",
    "    \n",
    "#     dfmap_2[index,:] = np.squeeze(x, axis=0)\n",
    "\n",
    "# # stop time\n",
    "# tf_dfmap = time.time()    \n",
    "# tt_dfmap = tf_dfmap - ti_dfmap\n",
    "# print()\n",
    "# print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_dfmap)))\n",
    "# print()\n",
    "# print(\"Tamaño en memoria de la matriz de características profundas: {0:.2f}Mb\".format(getsizeof(dfmap_2)/float(1<<20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(dfmap, open('../data/output_convnet/inception_dfmap_pickle', 'wb'))\n",
    "\n",
    "# print(os.path.getsize(\"../data/output_convnet/VGG16_dfmap_no_relU_pickle\")/float(1<<20)) #156 mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "PATH_TO_DF = \"../data/output_convnet\"\n",
    "DF_MAP = \"inception_dfmap_pickle\"\n",
    "\n",
    "\n",
    "dfmap = pickle.load(open(os.path.join(PATH_TO_DF,DF_MAP),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "features_arr = np.array(dfmap)\n",
    "features_arr = np.array(features_arr.T) # transpose to pca\n",
    "labels_arr = np.array(img_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# init time\n",
    "ti_pca = time.time()\n",
    "# calcs\n",
    "features_model_pca = PCA().fit(features_arr) # model\n",
    "\n",
    "# plot\n",
    "plt.plot(np.cumsum(features_model_pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "\n",
    "plt.savefig(os.path.join(PATH_TO_DF, \"inception_pca.png\"), bbox_inches='tight') # png 70kb vs jpg 135 kb\n",
    "plt.show()\n",
    "\n",
    "# stop time\n",
    "tf_pca = time.time()\n",
    "tt_pca = tf_pca - ti_pca\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(0.7,1,0.05):\n",
    "    print(\"With %d features we get %f of cumulative explicative variance.\" % \\\n",
    "          (np.argmax(features_model_pca.explained_variance_ratio_.cumsum() > i), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init time\n",
    "ti_pca = time.time()\n",
    "\n",
    "features_model_pca = PCA(n_components=400) # fixed number of features 95%\n",
    "features_model_pca.fit(features_arr)\n",
    "\n",
    "# stop time\n",
    "tf_pca = time.time()\n",
    "tt_pca = tf_pca - ti_pca\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_pca)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose again the matrix\n",
    "pickle.dump(features_model_pca.components_.T,\\\n",
    "            open(os.path.join(PATH_TO_DF,\"inception_dfmap_pca_pickle\"), 'wb')) # without relU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_MAP = \"inception_dfmap_pickle\" # original data no relU no pca\n",
    "DF_MAP_pca = \"inception_dfmap_pca_pickle\" # original data no relU with pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DF = \"../data/output_convnet\"\n",
    "DF_MAP_PCA = \"inception_dfmap_pca_pickle\"\n",
    "\n",
    "dfmap_pca = pickle.load(open(os.path.join(PATH_TO_DF,DF_MAP_PCA),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_arr = np.array(dfmap)\n",
    "features_arr_pca = np.array(dfmap_pca)\n",
    "\n",
    "labels_arr = np.array(img_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deep features - deep feature map\")\n",
    "print()\n",
    "print(\"Structure dims: {0:d} x {1:d}\".format(features_arr.shape[0], features_arr.shape[1]))\n",
    "print()\n",
    "print(\"N of deep features arrays (images): \", features_arr.shape[0])\n",
    "print()\n",
    "print(\"N of deep features: \",features_arr.shape[1])\n",
    "print()\n",
    "print(features_arr) # array with deep features\n",
    "print()\n",
    "print()\n",
    "print(\"Deep features PCA- deep feature map\")\n",
    "print()\n",
    "print(\"Structure dims: {0:d} x {1:d}\".format(features_arr_pca.shape[0], features_arr_pca.shape[1]))\n",
    "print()\n",
    "print(\"N of deep features arrays (images): \", features_arr_pca.shape[0])\n",
    "print()\n",
    "print(\"N of deep features: \",features_arr_pca.shape[1])\n",
    "print()\n",
    "print(features_arr_pca) # array with deep features\n",
    "print()\n",
    "print()\n",
    "print(\"Image's labels\")\n",
    "print()\n",
    "print(\"Structure: \", type(labels_arr))\n",
    "print()\n",
    "print(\"Nº of image labels (images): \", len(labels_arr))\n",
    "print()\n",
    "print(labels_arr) # img's label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "import time\n",
    "\n",
    "\n",
    "cv_skf = StratifiedKFold(n_splits=10, shuffle=False, random_state=42)\n",
    "# cv_kf = KFold(n_splits=5, shuffle=False, random_state=42)\n",
    "\n",
    "# svm\n",
    "SVM = LinearSVC()\n",
    "\n",
    "ti_svm = time.time()\n",
    "\n",
    "# data -> original dfmap no pca\n",
    "scores = cross_val_score(SVM, features_arr, labels_arr, cv=cv_skf, n_jobs = -1)\n",
    "# conf matrix\n",
    "y_pred = cross_val_predict(SVM,features_arr, labels_arr,cv=cv_skf, n_jobs = -1)\n",
    "conf_mat = confusion_matrix(labels_arr,y_pred)\n",
    "\n",
    "tf_svm = time.time()\n",
    "tt_svm = tf_svm - ti_svm\n",
    "\n",
    "\n",
    "ti_svm_pca = time.time()\n",
    "\n",
    "# data -> original dfmap pca\n",
    "scores_pca = cross_val_score(SVM, features_arr_pca, labels_arr, cv=cv_skf, n_jobs = -1)\n",
    "# conf matrix\n",
    "y_pred_pca = cross_val_predict(SVM,features_arr_pca, labels_arr,cv=cv_skf, n_jobs = -1)\n",
    "conf_mat_pca = confusion_matrix(labels_arr,y_pred_pca)\n",
    "\n",
    "tf_svm_pca = time.time()\n",
    "tt_svm_pca = tf_svm_pca - ti_svm_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc time\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_svm))) # no pca\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_svm_pca))) # pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature map sin reducción de dimensiones\")\n",
    "print()\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    print(\"acc fold nº {0:d}: {1:.2f}\".format(i+1,score*100))\n",
    "    \n",
    "print()\n",
    "print(\"media obtenida: {0:.2f}\".format(scores.mean()*100))\n",
    "\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature map con reducción de dimensiones\")\n",
    "print()\n",
    "\n",
    "for i, score in enumerate(scores_pca):\n",
    "    print(\"acc fold nº {0:d}: {1:.2f}\".format(i+1,score*100))\n",
    "    \n",
    "print()\n",
    "print(\"media obtenida: {0:.2f}\".format(scores_pca.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',size=20)\n",
    "    plt.xlabel('Predicted label',size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_IMG = \"../data/input_dataset\"\n",
    "class_names = sorted([folder for folder in os.listdir(PATH_TO_IMG)\n",
    "                      if os.path.isdir(os.path.join(PATH_TO_IMG, folder))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 6))\n",
    "\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "plot_confusion_matrix(conf_mat, classes=class_names,\n",
    "                      title='Confusion matrix no pca, without normalization')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plot_confusion_matrix(conf_mat, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix no pca')\n",
    "\n",
    "plt.savefig(os.path.join(PATH_TO_DF, \"inception_svm_no_pca.png\"), bbox_inches='tight') # png 70kb vs jpg 135 kb\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 6))\n",
    "\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "plot_confusion_matrix(conf_mat_pca, classes=class_names,\n",
    "                      title='Confusion matrix with pca, without normalization')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plot_confusion_matrix(conf_mat_pca, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix with pca')\n",
    "\n",
    "plt.savefig(os.path.join(PATH_TO_DF, \"inception_svm_pca.png\"), bbox_inches='tight') # png 70kb vs jpg 135 kb\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# DTC\n",
    "DTC = DecisionTreeClassifier()\n",
    "\n",
    "ti_dtc = time.time()\n",
    "\n",
    "# data -> original dfmap no pca\n",
    "scores = cross_val_score(DTC, features_arr, labels_arr, cv=cv_skf, n_jobs = -1)\n",
    "# conf matrix\n",
    "y_pred = cross_val_predict(DTC,features_arr, labels_arr,cv=cv_skf, n_jobs = -1)\n",
    "conf_mat = confusion_matrix(labels_arr,y_pred)\n",
    "\n",
    "tf_dtc = time.time()\n",
    "tt_dtc = tf_dtc - ti_dtc\n",
    "\n",
    "\n",
    "ti_dtc_pca = time.time()\n",
    "\n",
    "# data -> original dfmap pca\n",
    "scores_pca = cross_val_score(DTC, features_arr_pca, labels_arr, cv=cv_skf, n_jobs = -1)\n",
    "# conf matrix\n",
    "y_pred_pca = cross_val_predict(DTC,features_arr_pca, labels_arr,cv=cv_skf, n_jobs = -1)\n",
    "conf_mat_pca = confusion_matrix(labels_arr,y_pred_pca)\n",
    "\n",
    "tf_dtc_pca = time.time()\n",
    "tt_dtc_pca = tf_dtc_pca - ti_dtc_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc time\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_dtc))) # no pca\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_dtc_pca))) # pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature map sin reducción de dimensiones\")\n",
    "print()\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    print(\"acc fold nº {0:d}: {1:.2f}\".format(i+1,score*100))\n",
    "    \n",
    "print()\n",
    "print(\"media obtenida: {0:.2f}\".format(scores.mean()*100))\n",
    "\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature map con reducción de dimensiones\")\n",
    "print()\n",
    "\n",
    "for i, score in enumerate(scores_pca):\n",
    "    print(\"acc fold nº {0:d}: {1:.2f}\".format(i+1,score*100))\n",
    "    \n",
    "print()\n",
    "print(\"media obtenida: {0:.2f}\".format(scores_pca.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 6))\n",
    "\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "plot_confusion_matrix(conf_mat, classes=class_names,\n",
    "                      title='Confusion matrix no pca, without normalization')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plot_confusion_matrix(conf_mat, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix no pca')\n",
    "\n",
    "plt.savefig(os.path.join(PATH_TO_DF, \"inception_dtc_no_pca.png\"), bbox_inches='tight') # png 70kb vs jpg 135 kb\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 6))\n",
    "\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "plot_confusion_matrix(conf_mat_pca, classes=class_names,\n",
    "                      title='Confusion matrix with pca, without normalization')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plot_confusion_matrix(conf_mat_pca, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix with pca')\n",
    "\n",
    "plt.savefig(os.path.join(PATH_TO_DF, \"inception_dtc_pca.png\"), bbox_inches='tight') # png 70kb vs jpg 135 kb\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# RFC\n",
    "RFC = RandomForestClassifier()\n",
    "\n",
    "ti_rfc = time.time()\n",
    "\n",
    "# data -> original dfmap no pca\n",
    "scores = cross_val_score(RFC, features_arr, labels_arr, cv=cv_skf, n_jobs = -1)\n",
    "# conf matrix\n",
    "y_pred = cross_val_predict(RFC,features_arr, labels_arr,cv=cv_skf, n_jobs = -1)\n",
    "conf_mat = confusion_matrix(labels_arr,y_pred)\n",
    "\n",
    "tf_rfc = time.time()\n",
    "tt_rfc = tf_rfc - ti_rfc\n",
    "\n",
    "\n",
    "ti_rfc_pca = time.time()\n",
    "\n",
    "# data -> original dfmap pca\n",
    "scores_pca = cross_val_score(RFC, features_arr_pca, labels_arr, cv=cv_skf, n_jobs = -1)\n",
    "# conf matrix\n",
    "y_pred_pca = cross_val_predict(RFC,features_arr_pca, labels_arr,cv=cv_skf, n_jobs = -1)\n",
    "conf_mat_pca = confusion_matrix(labels_arr,y_pred_pca)\n",
    "\n",
    "tf_rfc_pca = time.time()\n",
    "tt_rfc_pca = tf_rfc_pca - ti_rfc_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc time\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_rfc))) # no pca\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_rfc_pca))) # pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature map sin reducción de dimensiones\")\n",
    "print()\n",
    "\n",
    "for i, score in enumerate(scores):\n",
    "    print(\"acc fold nº {0:d}: {1:.2f}\".format(i+1,score*100))\n",
    "    \n",
    "print()\n",
    "print(\"media obtenida: {0:.2f}\".format(scores.mean()*100))\n",
    "\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature map con reducción de dimensiones\")\n",
    "print()\n",
    "\n",
    "for i, score in enumerate(scores_pca):\n",
    "    print(\"acc fold nº {0:d}: {1:.2f}\".format(i+1,score*100))\n",
    "    \n",
    "print()\n",
    "print(\"media obtenida: {0:.2f}\".format(scores_pca.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 6))\n",
    "\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "plot_confusion_matrix(conf_mat, classes=class_names,\n",
    "                      title='Confusion matrix no pca, without normalization')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plot_confusion_matrix(conf_mat, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix no pca')\n",
    "\n",
    "plt.savefig(os.path.join(PATH_TO_DF, \"inception_rfc_no_pca.png\"), bbox_inches='tight') # png 70kb vs jpg 135 kb\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 6))\n",
    "\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "plot_confusion_matrix(conf_mat_pca, classes=class_names,\n",
    "                      title='Confusion matrix with pca, without normalization')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plot_confusion_matrix(conf_mat_pca, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix with pca')\n",
    "\n",
    "plt.savefig(os.path.join(PATH_TO_DF, \"inception_rfc_pca.png\"), bbox_inches='tight') # png 70kb vs jpg 135 kb\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
