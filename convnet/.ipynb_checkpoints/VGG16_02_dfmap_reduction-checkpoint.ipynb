{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejoras\n",
    "\n",
    "Relativas a la posible mejora de los resultados analíticos:\n",
    "\n",
    "* Reducción de dimensiones mediante selección de características profundas u otras técnicas\n",
    "\n",
    "Relativas a la mejora de los códigos Python:\n",
    "\n",
    "* mejora de código en general y estructuras\n",
    "* mejora lectura archivo img_labels\n",
    "* lectura archivo h5\n",
    "* control de errores en cada parte del proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_LABELS_FILE = \"../data/output_dataset\"\n",
    "LABELS = \"img_labels\"\n",
    "\n",
    "PATH_TO_DF = \"../data/output_convnet\"\n",
    "DF_MAP = \"VGG16_dfmap_relU_pickle\" # with relU activation function\n",
    "# DF_MAP = \"VGG16_dfmap_no_relU_pickle\" # without activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "labels = []\n",
    "\n",
    "with open(os.path.join(PATH_TO_LABELS_FILE,LABELS),'r') as f_img_labels:\n",
    "    for line in f_img_labels:\n",
    "        labels.append(line[:-1])\n",
    "\n",
    "features = pickle.load(open(os.path.join(PATH_TO_DF,DF_MAP),'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "features_arr = np.array(features)\n",
    "labels_arr = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep features - deep feature map\n",
      "\n",
      "Structure dims: 5000 x 4096\n",
      "\n",
      "N of deep features arrays (images):  5000\n",
      "\n",
      "N of deep features:  4096\n",
      "\n",
      "[[ 1.4363389   0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.47403866  0.          0.        ]\n",
      " [ 7.5820446   0.          0.         ...,  9.63796329  0.          0.        ]\n",
      " ..., \n",
      " [ 0.          1.40950263  0.         ...,  0.21009497  0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.99256498  0.          0.        ]\n",
      " [ 0.          0.57167876  0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      "\n",
      "Image's labels\n",
      "\n",
      "Structure:  <class 'numpy.ndarray'>\n",
      "\n",
      "Nº of image labels (images):  5000\n",
      "\n",
      "['01_TUMOR' '01_TUMOR' '01_TUMOR' ..., '08_EMPTY' '08_EMPTY' '08_EMPTY']\n"
     ]
    }
   ],
   "source": [
    "print(\"Deep features - deep feature map\")\n",
    "print()\n",
    "print(\"Structure dims: {0:d} x {1:d}\".format(features_arr.shape[0], features_arr.shape[1]))\n",
    "print()\n",
    "print(\"N of deep features arrays (images): \", features_arr.shape[0])\n",
    "print()\n",
    "print(\"N of deep features: \",features_arr.shape[1])\n",
    "print()\n",
    "print(features) # array with deep features\n",
    "print()\n",
    "print()\n",
    "print(\"Image's labels\")\n",
    "print()\n",
    "print(\"Structure: \", type(labels_arr))\n",
    "print()\n",
    "print(\"Nº of image labels (images): \", len(labels_arr))\n",
    "print()\n",
    "print(labels_arr) # img's label\n",
    "\n",
    "features_arr = features_arr.T # in order to reduce the columns number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# init time\n",
    "ti_pca = time.time()\n",
    "# calcs\n",
    "features_model_pca = PCA().fit(features_arr) # model\n",
    "\n",
    "# plot\n",
    "plt.plot(np.cumsum(features_model_pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "\n",
    "# stop time\n",
    "tf_pca = time.time()\n",
    "tt_pca = tf_pca - ti_pca\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_pca)))\n",
    "print(\"Tamaño en memoria del modelo: {0:.2f}Mb\".format(getsizeof(features_model_pca)/float(1<<20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 53 features we get 0.700000 of cumulative explicative variance.\n",
      "With 90 features we get 0.750000 of cumulative explicative variance.\n",
      "With 159 features we get 0.800000 of cumulative explicative variance.\n",
      "With 291 features we get 0.850000 of cumulative explicative variance.\n",
      "With 539 features we get 0.900000 of cumulative explicative variance.\n",
      "With 1028 features we get 0.950000 of cumulative explicative variance.\n",
      "With 0 features we get 1.000000 of cumulative explicative variance.\n"
     ]
    }
   ],
   "source": [
    "# weird output in the final step of arange()\n",
    "\n",
    "for i in np.arange(0.7,1,0.05):\n",
    "    print(\"With %d features we get %f of cumulative explicative variance.\" % \\\n",
    "          (np.argmax(features_model_pca.explained_variance_ratio_.cumsum() > i), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=1028, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_model_pca = PCA(n_components=1028) # fixed number of features\n",
    "features_model_pca.fit(features_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose again the matrix\n",
    "pickle.dump(features_model_pca.components_.T,\\\n",
    "            open(os.path.join(PATH_TO_DF,\"VGG16_dfmap_relU_pca_pickle\"), 'wb')) # with relU 33.7Mb\n",
    "# pickle.dump(features_model_pca.components_.T,\\\n",
    "#             open(os.path.join(PATH_TO_DF,\"VGG16_dfmap_no_relU_pca_pickle\"), 'wb')) # without relU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data with relU\n",
    "       * file 33.7Mb\n",
    "       * time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
