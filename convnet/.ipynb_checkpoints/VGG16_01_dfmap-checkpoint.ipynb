{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mejoras\n",
    "\n",
    "* mejorar el codigo y adaptarlo a la api2 de keras\n",
    "* mejorar la lectura/escritura de los ficheros\n",
    "* mejora de la organizacion de los ficheros de los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_labels & img_list - completed path to file\n",
    "PATH_TO_LABELS_FILE = \"../data/output_dataset/img_labels\"\n",
    "PATH_TO_PATHS_FILE = \"../data/output_dataset/img_list\"\n",
    "\n",
    "# output convnet files path\n",
    "PATH_TO_OUTPUT = \"../data/output_convnet\"\n",
    "\n",
    "img_labels = []\n",
    "img_list = []\n",
    "\n",
    "with open(PATH_TO_LABELS_FILE,'r') as f_img_labels:\n",
    "    for line in f_img_labels:\n",
    "        img_labels.append(line[:-1]) \n",
    "\n",
    "with open(PATH_TO_PATHS_FILE,'r') as f_img_list:\n",
    "    for line in f_img_list:\n",
    "        img_list.append(\"../data/\"+line[:-1]) # para eliminar el retorno de carro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fc1 layer with activation relU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, VGG16\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed to create session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1fca30ced8a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# if last command does not work properly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkeras_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"keras_weights/vgg16_weights_tf_dim_ordering_tf_kernels.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.6/site-packages/keras/applications/vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[1;32m    173\u001b[0m                                     \u001b[0mcache_subdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                                     file_hash='6d6bbae143d832006294945121d1f1fc')\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'theano'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mlayer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_all_kernels_in_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   2620\u001b[0m             \u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2622\u001b[0;31m             \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m   3141\u001b[0m                              ' elements.')\n\u001b[1;32m   3142\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3143\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2250\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2252\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m                 config = tf.ConfigProto(intra_op_parallelism_threads=num_thread,\n\u001b[1;32m    168\u001b[0m                                         allow_soft_placement=True)\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0m_SESSION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m     \"\"\"\n\u001b[0;32m-> 1482\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    620\u001b[0m           \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewDeprecatedSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_DeleteSessionOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed to create session."
     ]
    }
   ],
   "source": [
    "# base_model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "# if last command does not work properly\n",
    "keras_weights = \"keras_weights/vgg16_weights_tf_dim_ordering_tf_kernels.h5\"\n",
    "base_model = VGG16(include_top=True)\n",
    "model.load_weights(weights_path)\n",
    "\n",
    "# check for trainable layers\n",
    "# for i, layer in enumerate(base_model.layers):\n",
    "#     if layer.trainable:\n",
    "#         print(\"layer {0:s} is trainable\".format(layer.name))\n",
    "#     else:\n",
    "#         print(\"layer {0:s} is freezed\".format(layer.name))\n",
    "\n",
    "# model architecture\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sys import getsizeof\n",
    "\n",
    "deep_features = 4096\n",
    "VGG16_dfmap = np.empty((len(img_list),deep_features))\n",
    "\n",
    "# init time\n",
    "ti_dfmap_relU = time.time()\n",
    "\n",
    "for index, img_path in enumerate(img_list):\n",
    "    \n",
    "    index += 1\n",
    "    \n",
    "    if not index % 500:\n",
    "        print(\"image {0:d} processed\".format(index))\n",
    "\n",
    "    model = Model(input=base_model.input, output=base_model.get_layer('fc1').output)\n",
    "\n",
    "    # img preprocessing\n",
    "    img = image.load_img(img_path, target_size=(224, 224), interpolation='lanczos')\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    # model predict\n",
    "    x = model.predict(x)\n",
    "    \n",
    "    VGG16_dfmap[index,:] = np.squeeze(x, axis=0)\n",
    "\n",
    "# stop time\n",
    "tf_dfmap_relU = time.time()    \n",
    "tt_dfmap_relU = tf_dfmap_relU - ti_dfmap_relU\n",
    "print()\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_dfmap_relU)))\n",
    "print()\n",
    "print(\"Tamaño en memoria de la matriz de características profundas: {0:.2f}Mb\".format(getsizeof(VGG16_dfmap)/float(1<<20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the relU function\n",
    "VGG16_dfmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "# init time\n",
    "ti_h5file = time.time()\n",
    "\n",
    "hf = h5py.File(os.path.join(PATH_TO_OUTPUT, 'VGG16_dfmap_relU.h5'), 'w')\n",
    "hf.create_dataset('dfmap', data=VGG16_dfmap) #163.8 mb no compression at all\n",
    "# hf.create_dataset('dfmap', data=VGG16_dfmap,compression=\"gzip\", compression_opts=9) # 32.4 mb\n",
    "hf.close()\n",
    "\n",
    "# stop time\n",
    "tf_h5file = time.time()\n",
    "tt_h5file = tf_h5file - ti_h5file\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_h5file)))\n",
    "\n",
    "# import pickle\n",
    "# pickle.dump(VGG16_dfmap, open('../data/output_convnet/VGG16_dfmap_relU_pickle', 'wb'))\n",
    "\n",
    "# print(os.path.getsize(\"../data/output_convnet/VGG16_dfmap_relU_pickle\")/float(1<<20)) #156 mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file information\n",
    "import h5py\n",
    "\n",
    "hf = h5py.File(os.path.join(PATH_TO_OUTPUT, 'VGG16_dfmap_relU.h5'), 'r')\n",
    "print(hf.keys())\n",
    "n1 = hf.get('dfmap')\n",
    "print(type(n1))\n",
    "print(n1)\n",
    "\n",
    "n1 = np.array(n1)\n",
    "print(type(n1))\n",
    "print(n1.shape)\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fc1 layer with no activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/keras-team/keras/blob/master/keras/applications/vgg16.py # estructura implementada en keras\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224,3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# top layer of the VGG net\n",
    "model.add(Dense(4096, activation=None,name=\"fc1\")) # activation function set to none\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation=None,name=\"fc2\")) # activation function set to none\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"keras_weights/vgg16_weights_tf_dim_ordering_tf_kernels.h5\")\n",
    "learning_rate = 0.01\n",
    "model.compile(optimizer=SGD(lr=learning_rate), loss='categorical_crossentropy')\n",
    "\n",
    "# check for trainable layers\n",
    "# for i, layer in enumerate(model.layers):\n",
    "#     if layer.trainable:\n",
    "#         print(\"layer {0:s} is trainable\".format(layer.name))\n",
    "#     else:\n",
    "#         print(\"layer {0:s} is freezed\".format(layer.name))\n",
    "        \n",
    "# for i, layer in enumerate(model.layers):\n",
    "#     print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_dfmap_no_relU = np.empty((len(img_list),deep_features))\n",
    "\n",
    "# init time\n",
    "ti_dfmap_no_relU = time.time()\n",
    "\n",
    "for index, img_path in enumerate(img_list):\n",
    "    \n",
    "    if not index % 500:\n",
    "        print(\"image {0:d} processed\".format(index))\n",
    "\n",
    "    model_no_relU = Model(input=model.input, output=model.get_layer('fc1').output)\n",
    "\n",
    "    # pre procesado de imagen\n",
    "    img = image.load_img(img_path, target_size=(224, 224), interpolation='lanczos')\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    # model predict\n",
    "    x = model_no_relU.predict(x)\n",
    "    \n",
    "    VGG16_dfmap_no_relU[index,:] = np.squeeze(x, axis=0)\n",
    "\n",
    "# stop time\n",
    "tf_dfmap_no_relU = time.time()    \n",
    "tt_dfmap_no_relU = tf_dfmap_no_relU - ti_dfmap_no_relU\n",
    "print()\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_dfmap_no_relU)))\n",
    "print()\n",
    "print(\"Tamaño en memoria de la matriz de características profundas: {0:.2f}Mb\".format(getsizeof(VGG16_dfmap_no_relU)/float(1<<20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the activation function set to none\n",
    "VGG16_dfmap_no_relU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init time\n",
    "ti_h5file_no_relU = time.time()\n",
    "\n",
    "hf = h5py.File(os.path.join(PATH_TO_OUTPUT, 'VGG16_dfmap_no_relU.h5'), 'w')\n",
    "hf.create_dataset('dfmap', data=VGG16_dfmap) #163.8 mb\n",
    "# hf.create_dataset('dfmap', data=VGG16_dfmap_no_relU,compression=\"gzip\", compression_opts=9) # 32.4 mb\n",
    "hf.close()\n",
    "\n",
    "# stop time\n",
    "tf_h5file_no_relU = time.time()\n",
    "tt_h5file_no_relU = tf_h5file_no_relU - ti_h5file_no_relU\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_h5file_no_relU)))\n",
    "\n",
    "# import pickle\n",
    "# pickle.dump(VGG16_dfmap_no_relU, open('../data/output_convnet/VGG16_dfmap_no_relU_pickle', 'wb'))\n",
    "\n",
    "# print(os.path.getsize(\"../data/output_convnet/VGG16_dfmap_no_relU_pickle\")/float(1<<20)) #156 mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File(os.path.join(PATH_TO_OUTPUT, 'VGG16_dfmap_no_relU.h5'), 'r')\n",
    "print(hf.keys())\n",
    "n1 = hf.get('dfmap')\n",
    "print(type(n1))\n",
    "print(n1)\n",
    "\n",
    "n1 = np.array(n1)\n",
    "print(type(n1))\n",
    "print(n1.shape)\n",
    "\n",
    "hf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
