{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red convolucional - VGG16\n",
    "\n",
    "\n",
    "0. Organización\n",
    "1. Introducción arquitectura VGG16.\n",
    "2. Carga de imágenes.\n",
    "    * Estructura listado\n",
    "3. Implementación arquitectura.\n",
    "    * Keras\n",
    "    * Capas\n",
    "    * Arquitectura pre entrenada - Imagenet \n",
    "4. Transferencia de aprendizaje: descriptores profundos.\n",
    "    * Pre procesado de imágenes\n",
    "    * Reducción dimensionalidad\n",
    "    * Clasificación\n",
    "    * Medidas de desempeño\n",
    "5. Transferencia de aprendizaje: Fine tuning.\n",
    "    * Medidas de desempeño\n",
    "6. Mejoras\n",
    "7. Resultados - Tablas. Resumen\n",
    "8. Discusión resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estructura de archivos y carpetas, qué parte se encuentra en cada archivo y explicacion de la división que se ha hecho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción arquitectura VGG_16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-16.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta parte se enlaza con el notebook info_dataset explicación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Abrimos los archivos generados con los nombres de categoria y sus respectivos paths..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '../system/functions.ipynb' # import util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time # execution time no the best accurate but the easiest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_labels & img_list - completed path to file\n",
    "PATH_TO_LABELS_FILE = \"../data/output_dataset/img_labels\"\n",
    "PATH_TO_PATHS_FILE = \"../data/output_dataset/img_list\"\n",
    "\n",
    "# output convnet files path\n",
    "PATH_TO_OUTPUT = \"../data/output_convnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_labels = []\n",
    "img_list = []\n",
    "\n",
    "'''\n",
    "para eliminar el retorno de carro --> [:-1]\n",
    "para encontrar el path correcto hay que transformar \n",
    "la ruta puesto que este fichero y desde el que se crea tienen rutas distintas\n",
    "'''\n",
    "# input_dataset/01_TUMOR/2642_CRC-Prim-HE-07_025.tif_Row_1351_Col_601.tif\n",
    "\n",
    "with open(PATH_TO_LABELS_FILE,'r') as f_img_labels:\n",
    "    for line in f_img_labels:\n",
    "        img_labels.append(line[:-1]) \n",
    "\n",
    "with open(PATH_TO_PATHS_FILE,'r') as f_img_list:\n",
    "    for line in f_img_list:\n",
    "        img_list.append(\"../data/\"+line[:-1]) # para eliminar el retorno de carro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 aproximacion fc1 y fc2 activacion relU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge keras hecho en configuración carpeta system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, VGG16\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=True) # clase implementada en keras\n",
    "# for i, layer in enumerate(base_model.layers):\n",
    "#     print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_img --> sustituye a mi funcion de reescalado basica utilizo directamente la de keras\n",
    "\n",
    "explicacion de la siguiente celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deep_features = 4096\n",
    "VGG16_dfmap = np.empty((len(img_list),deep_features))\n",
    "\n",
    "# por si acaso\n",
    "# VGG16_img_labels = [] # no hace falta\n",
    "\n",
    "# import re # no hace falta\n",
    "\n",
    "ti_dfmap_relU = time()\n",
    "\n",
    "for index, img_path in enumerate(img_list):\n",
    "    \n",
    "    if (index == 1000) or (index == 2000) or (index == 3000) or (index == 4000):\n",
    "        print(\"image {0:d} processed\".format(index))\n",
    "\n",
    "    model = Model(input=base_model.input, output=base_model.get_layer('fc1').output)\n",
    "\n",
    "    #pre procesado de imagen\n",
    "    img = image.load_img(img_path, target_size=(224, 224), interpolation='lanczos')\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    # model predict\n",
    "    x = model.predict(x)\n",
    "#     x.shape\n",
    "    \n",
    "    # store the data\n",
    "    VGG16_dfmap[index,:] = np.squeeze(x, axis=0)\n",
    "#     VGG16_img_labels.append(re.split('_\\d+',img_path.split('/')[3])[0]) # tenemos la lista en txt\n",
    "    \n",
    "\n",
    "tf_dfmap_relU = time()    \n",
    "\n",
    "tt_dfmap_relU = tf_dfmap_relU - ti_dfmap_relU\n",
    "\n",
    "VGG16_img_labels = np.array(VGG16_img_labels) # conversion a array para poder guardarlo en h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "print(getsizeof(VGG16_dfmap)/float(1<<20)) # aprox mem size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "archivos guardados con relU activation en fc1 y fc2, y con h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "\n",
    "hf = h5py.File(os.path.join(PATH_TO_OUTPUT, 'VGG16_dfmap_relU.h5'), 'w')\n",
    "\n",
    "ti_h5file = time()\n",
    "\n",
    "# hf.create_dataset('dfmap', data=VGG16_dfmap) #163.8 mb\n",
    "hf.create_dataset('dfmap', data=VGG16_dfmap,compression=\"gzip\", compression_opts=9) # 32.4 mb\n",
    "\n",
    "tf_h5file = time()\n",
    "\n",
    "tt_htfile = tf_h5file - ti_h5file\n",
    "\n",
    "hf.close()\n",
    "\n",
    "#tiempo\n",
    "print(\"tiempo\", tt_htfile) #en segundos\n",
    "\n",
    "#tamaño fichero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "hf2 = h5py.File(os.path.join(PATH_TO_OUTPUT, 'VGG16_dfmap_relU.h5'), 'r')\n",
    "print(hf2.keys())\n",
    "n1 = hf2.get('dfmap')\n",
    "print(type(n1))\n",
    "print(n1)\n",
    "\n",
    "n1 = np.array(n1)\n",
    "print(type(n1))\n",
    "print(n1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "archivos guardados con relU activation en fc1 y fc2, y con pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(VGG16_dfmap, open('../data/output_convnet/VGG16_dfmap_relU_pickle', 'wb'))\n",
    "# pickle.dump(VGG16_img_labels, open('../data/output_convnet/VGG16_img_labels_relU_pickle', 'wb'))\n",
    "\n",
    "#tamaño fichero\n",
    "\n",
    "print(os.path.getsize(\"../data/output_convnet/VGG16_dfmap_relU_pickle\")/float(1<<20)) #156 mb\n",
    "print(os.path.getsize(\"../data/output_convnet/VGG16_img_labels_relU_pickle\")) # 94 kb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 aproximacion fc1 y fc2 sin activacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos un nuevo modelo con fc1 y fc2 sin activacion, y cargamos los pesos de la red completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creamos el modelo usando el metodo sequential y siguiendo la estructura encontrada en "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprobar si el modelo es vgg16 como el implementado en keras\n",
    "# https://github.com/keras-team/keras/blob/master/keras/applications/vgg16.py # estructura implementada en keras\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224,3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#top layer of the VGG net\n",
    "model.add(Dense(4096, activation=None,name=\"fc1\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation=None,name=\"fc2\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "# model.summary() # lo oculto por ahora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cargamos los pesos --> estos pesos dependen de la funcion de activacion? es decir, cargo los pesos con los que se entreno vgg16 con el dataset imaginet en la definicion de la clase VGG16 de keras, tiene implicito q se calcula con activacion relU? esa es la gran duda\n",
    "\n",
    "otra forma seria cargar la red con la clase vgg16 de keras sin top, cargar los pesos sin top, congelar esas capas y entrenar sin activacion la capa dense fc1 sin activacion\n",
    "\n",
    "si en el paso anterior creamos otro modelo secuencial con estructura similar a vgg16 pero solo con 8 clases softmax obtendremos bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"keras_weights/vgg16_weights_tf_dim_ordering_tf_kernels.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/models/sequential/#compile\n",
    "\n",
    "model.compile(optimizer=SGD(), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "las capas de la red implementada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, layer in enumerate(model.layers):\n",
    "#     print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Mejoras\n",
    "\n",
    "* mejorar la lectura de los ficheros\n",
    "* repasar los imports y eliminar los repetidos\n",
    "* mejora de la organizacion de los ficheros de los pesos\n",
    "* entender los distintos pesos (3 tipos imagenet, los precargados en ficheros h5 y los que se bajan la primera vez)\n",
    "* mejora guardado de archivos con hd5py o similares"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
