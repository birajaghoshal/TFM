{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.applications.vgg19 import preprocess_input, VGG19\n",
    "# from keras import applications\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DF = \"../data/output_convnet/VGG19\"\n",
    "\n",
    "img_width = 150\n",
    "img_height = 150\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "train_data_dir = \"train\"  \n",
    "validation_data_dir = \"validation\"\n",
    "test_data_dir = \"test\"\n",
    "\n",
    "n_train_samples = 3000\n",
    "n_validation_samples = 1000\n",
    "n_test_samples = 1000\n",
    "n_classes = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 images belonging to 8 classes.\n",
      "Found 1000 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "# https://b/log.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "# saving bottleneck features\n",
    "import time\n",
    "\n",
    "ti_bn_features = time.time()\n",
    "\n",
    "model = VGG19(include_top=False, weights='imagenet')\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "'''\n",
    "https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py#L1002\n",
    "if PIL version 1.1.3 interpolation = 'lanczos'\n",
    "else interpolation = 'bicubic' \n",
    "''' \n",
    "\n",
    "# train\n",
    "generator = datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = None,\n",
    "    shuffle = False,\n",
    "    interpolation = 'lanczos')\n",
    "\n",
    "# important step in order to get the exact number\n",
    "max_queue_size_train = int(math.ceil(n_train_samples / batch_size))\n",
    "\n",
    "bnfeatures_train = model.predict_generator(\n",
    "    generator, max_queue_size_train)\n",
    "\n",
    "np.save('../data/output_convnet/VGG19/VGG19_bnfeatures_train.npy', bnfeatures_train)\n",
    "\n",
    "# validation\n",
    "generator = datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False,\n",
    "    interpolation = 'lanczos')\n",
    "\n",
    "max_queue_size_val = int(math.ceil(n_validation_samples / batch_size))\n",
    "\n",
    "   \n",
    "bnfeatures_val = model.predict_generator(  \n",
    "     generator, max_queue_size_val)\n",
    "\n",
    "np.save('../data/output_convnet/VGG19/VGG19_bnfeatures_val.npy', bnfeatures_val)\n",
    "\n",
    "\n",
    "tf_bn_features = time.time()    \n",
    "tt_bn_features = tf_bn_features - ti_bn_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:36\n"
     ]
    }
   ],
   "source": [
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_bn_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3000 images belonging to 8 classes.\n",
      "Found 1000 images belonging to 8 classes.\n",
      "Train on 3000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "3000/3000 [==============================] - 1s 463us/step - loss: 1.6777 - acc: 0.3997 - val_loss: 1.2578 - val_acc: 0.7020\n",
      "Epoch 2/50\n",
      "3000/3000 [==============================] - 1s 302us/step - loss: 1.2549 - acc: 0.5703 - val_loss: 1.0334 - val_acc: 0.7300\n",
      "Epoch 3/50\n",
      "3000/3000 [==============================] - 1s 289us/step - loss: 1.0956 - acc: 0.6353 - val_loss: 0.8970 - val_acc: 0.7790\n",
      "Epoch 4/50\n",
      "3000/3000 [==============================] - 1s 320us/step - loss: 0.9858 - acc: 0.6660 - val_loss: 0.8247 - val_acc: 0.7760\n",
      "Epoch 5/50\n",
      "3000/3000 [==============================] - 1s 314us/step - loss: 0.8989 - acc: 0.6980 - val_loss: 0.7618 - val_acc: 0.7970\n",
      "Epoch 6/50\n",
      "3000/3000 [==============================] - 1s 303us/step - loss: 0.8537 - acc: 0.7187 - val_loss: 0.7283 - val_acc: 0.7840\n",
      "Epoch 7/50\n",
      "3000/3000 [==============================] - 1s 292us/step - loss: 0.8043 - acc: 0.7350 - val_loss: 0.6914 - val_acc: 0.7840\n",
      "Epoch 8/50\n",
      "3000/3000 [==============================] - 1s 305us/step - loss: 0.7716 - acc: 0.7407 - val_loss: 0.6673 - val_acc: 0.8030\n",
      "Epoch 9/50\n",
      "3000/3000 [==============================] - 1s 290us/step - loss: 0.7415 - acc: 0.7587 - val_loss: 0.6242 - val_acc: 0.8070\n",
      "Epoch 10/50\n",
      "3000/3000 [==============================] - 1s 295us/step - loss: 0.7074 - acc: 0.7650 - val_loss: 0.6199 - val_acc: 0.8080\n",
      "Epoch 11/50\n",
      "3000/3000 [==============================] - 1s 318us/step - loss: 0.6972 - acc: 0.7750 - val_loss: 0.6395 - val_acc: 0.7830\n",
      "Epoch 12/50\n",
      "3000/3000 [==============================] - 1s 308us/step - loss: 0.6743 - acc: 0.7820 - val_loss: 0.5731 - val_acc: 0.8170\n",
      "Epoch 13/50\n",
      "3000/3000 [==============================] - 1s 304us/step - loss: 0.6525 - acc: 0.7873 - val_loss: 0.5805 - val_acc: 0.8120\n",
      "Epoch 14/50\n",
      "3000/3000 [==============================] - 1s 318us/step - loss: 0.6248 - acc: 0.7987 - val_loss: 0.5710 - val_acc: 0.8140\n",
      "Epoch 15/50\n",
      "3000/3000 [==============================] - 1s 312us/step - loss: 0.6234 - acc: 0.7957 - val_loss: 0.5552 - val_acc: 0.8110\n",
      "Epoch 16/50\n",
      "3000/3000 [==============================] - 1s 313us/step - loss: 0.5970 - acc: 0.8127 - val_loss: 0.5417 - val_acc: 0.8200\n",
      "Epoch 17/50\n",
      "3000/3000 [==============================] - 1s 333us/step - loss: 0.5998 - acc: 0.8010 - val_loss: 0.5403 - val_acc: 0.8140\n",
      "Epoch 18/50\n",
      "3000/3000 [==============================] - 1s 324us/step - loss: 0.6007 - acc: 0.8010 - val_loss: 0.5324 - val_acc: 0.8230\n",
      "Epoch 19/50\n",
      "3000/3000 [==============================] - 1s 321us/step - loss: 0.5756 - acc: 0.8137 - val_loss: 0.5246 - val_acc: 0.8220\n",
      "Epoch 20/50\n",
      "3000/3000 [==============================] - 1s 312us/step - loss: 0.5613 - acc: 0.8150 - val_loss: 0.5062 - val_acc: 0.8320\n",
      "Epoch 21/50\n",
      "3000/3000 [==============================] - 1s 309us/step - loss: 0.5658 - acc: 0.8093 - val_loss: 0.4966 - val_acc: 0.8300\n",
      "Epoch 22/50\n",
      "3000/3000 [==============================] - 1s 304us/step - loss: 0.5576 - acc: 0.8167 - val_loss: 0.5027 - val_acc: 0.8270\n",
      "Epoch 23/50\n",
      "3000/3000 [==============================] - 1s 300us/step - loss: 0.5386 - acc: 0.8260 - val_loss: 0.4976 - val_acc: 0.8350\n",
      "Epoch 24/50\n",
      "3000/3000 [==============================] - 1s 301us/step - loss: 0.5462 - acc: 0.8163 - val_loss: 0.4792 - val_acc: 0.8380\n",
      "Epoch 25/50\n",
      "3000/3000 [==============================] - 1s 306us/step - loss: 0.5227 - acc: 0.8227 - val_loss: 0.4859 - val_acc: 0.8270\n",
      "Epoch 26/50\n",
      "3000/3000 [==============================] - 1s 308us/step - loss: 0.5072 - acc: 0.8377 - val_loss: 0.4682 - val_acc: 0.8390\n",
      "Epoch 27/50\n",
      "3000/3000 [==============================] - 1s 344us/step - loss: 0.5169 - acc: 0.8243 - val_loss: 0.5016 - val_acc: 0.8120\n",
      "Epoch 28/50\n",
      "3000/3000 [==============================] - 1s 320us/step - loss: 0.5064 - acc: 0.8337 - val_loss: 0.4659 - val_acc: 0.8390\n",
      "Epoch 29/50\n",
      "3000/3000 [==============================] - 1s 297us/step - loss: 0.5064 - acc: 0.8310 - val_loss: 0.4822 - val_acc: 0.8280\n",
      "Epoch 30/50\n",
      "3000/3000 [==============================] - 1s 300us/step - loss: 0.4863 - acc: 0.8397 - val_loss: 0.4749 - val_acc: 0.8230\n",
      "Epoch 31/50\n",
      "3000/3000 [==============================] - 1s 309us/step - loss: 0.4832 - acc: 0.8387 - val_loss: 0.4738 - val_acc: 0.8340\n",
      "Epoch 32/50\n",
      "3000/3000 [==============================] - 1s 313us/step - loss: 0.4782 - acc: 0.8463 - val_loss: 0.4633 - val_acc: 0.8340\n",
      "Epoch 33/50\n",
      "3000/3000 [==============================] - 1s 303us/step - loss: 0.4704 - acc: 0.8487 - val_loss: 0.4598 - val_acc: 0.8380\n",
      "Epoch 34/50\n",
      "3000/3000 [==============================] - 1s 312us/step - loss: 0.4691 - acc: 0.8517 - val_loss: 0.4460 - val_acc: 0.8390\n",
      "Epoch 35/50\n",
      "3000/3000 [==============================] - 1s 308us/step - loss: 0.4611 - acc: 0.8480 - val_loss: 0.4755 - val_acc: 0.8300\n",
      "Epoch 36/50\n",
      "3000/3000 [==============================] - 1s 300us/step - loss: 0.4601 - acc: 0.8490 - val_loss: 0.4487 - val_acc: 0.8470\n",
      "Epoch 37/50\n",
      "3000/3000 [==============================] - 1s 338us/step - loss: 0.4669 - acc: 0.8473 - val_loss: 0.4378 - val_acc: 0.8400\n",
      "Epoch 38/50\n",
      "3000/3000 [==============================] - 1s 365us/step - loss: 0.4560 - acc: 0.8513 - val_loss: 0.4368 - val_acc: 0.8490\n",
      "Epoch 39/50\n",
      "3000/3000 [==============================] - 1s 317us/step - loss: 0.4448 - acc: 0.8553 - val_loss: 0.4319 - val_acc: 0.8560\n",
      "Epoch 40/50\n",
      "3000/3000 [==============================] - 1s 314us/step - loss: 0.4479 - acc: 0.8553 - val_loss: 0.4301 - val_acc: 0.8470\n",
      "Epoch 41/50\n",
      "3000/3000 [==============================] - 1s 308us/step - loss: 0.4426 - acc: 0.8533 - val_loss: 0.4235 - val_acc: 0.8540\n",
      "Epoch 42/50\n",
      "3000/3000 [==============================] - 1s 308us/step - loss: 0.4286 - acc: 0.8580 - val_loss: 0.4275 - val_acc: 0.8470\n",
      "Epoch 43/50\n",
      "3000/3000 [==============================] - 1s 314us/step - loss: 0.4285 - acc: 0.8587 - val_loss: 0.4208 - val_acc: 0.8500\n",
      "Epoch 44/50\n",
      "3000/3000 [==============================] - 1s 332us/step - loss: 0.4231 - acc: 0.8610 - val_loss: 0.4223 - val_acc: 0.8500\n",
      "Epoch 45/50\n",
      "3000/3000 [==============================] - 1s 319us/step - loss: 0.4204 - acc: 0.8587 - val_loss: 0.4305 - val_acc: 0.8350\n",
      "Epoch 46/50\n",
      "1088/3000 [=========>....................] - ETA: 0s - loss: 0.4121 - acc: 0.8603"
     ]
    }
   ],
   "source": [
    "# training top layer\n",
    "import os\n",
    "\n",
    "ti_bn_train = time.time()\n",
    "\n",
    "# we have to get the classes names so we build again a generator\n",
    "datagen_top_layer = ImageDataGenerator(rescale=1./255)  \n",
    "\n",
    "# train\n",
    "generator_top_layer = datagen_top_layer.flow_from_directory(\n",
    "    train_data_dir,  \n",
    "    target_size=(img_width, img_height),  \n",
    "    batch_size=batch_size,  \n",
    "    class_mode='categorical',  \n",
    "    shuffle=False,\n",
    "    interpolation = 'lanczos')  \n",
    "\n",
    "train_data = np.load('../data/output_convnet/VGG19/VGG19_bnfeatures_train.npy')\n",
    "\n",
    "# ref attribute classes --> https://keras.io/preprocessing/image/\n",
    "train_labels = generator_top_layer.classes # the key attribute\n",
    "train_labels = to_categorical(train_labels, num_classes=n_classes) # the key function\n",
    "\n",
    "# validation\n",
    "generator_top_layer = datagen_top_layer.flow_from_directory(  \n",
    "    validation_data_dir,  \n",
    "    target_size=(img_width, img_height),  \n",
    "    batch_size=batch_size,  \n",
    "    class_mode=None,  \n",
    "    shuffle=False,\n",
    "    interpolation = 'lanczos')  \n",
    "\n",
    "val_data = np.load('../data/output_convnet/VGG19/VGG19_bnfeatures_val.npy') \n",
    "\n",
    "val_labels = generator_top_layer.classes # the key attribute\n",
    "val_labels = to_categorical(val_labels, num_classes=n_classes) # the key function\n",
    "\n",
    "\n",
    "# top model, could be with a diff dense, optimizer, momentum -> https://keras.io/optimizers/\n",
    "model = Sequential()  \n",
    "model.add(Flatten(input_shape=train_data.shape[1:]))  \n",
    "model.add(Dense(256, activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(n_classes, activation='softmax'))  \n",
    "\n",
    "\n",
    "model.compile(optimizer=SGD(lr=0.001),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "\n",
    "historical_data = model.fit(train_data, train_labels,\n",
    "                    epochs=n_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(val_data, val_labels))  \n",
    "\n",
    "# h5py\n",
    "model.save_weights('../data/output_convnet/VGG19/VGG19_bn_model.h5')  \n",
    "\n",
    "tf_bn_train = time.time()    \n",
    "tt_bn_train = tf_bn_train - ti_bn_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_bn_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "hdf5_file = h5py.File('../data/output_convnet/VGG19/VGG19_bn_model.h5', mode='r')\n",
    "print(list(hdf5_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss, acc) = model.evaluate(val_data, val_labels, batch_size=batch_size, verbose=0)\n",
    "print(\"acc: {0:.2f}% - loss: {1:f}\".format(acc * 100, loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# historical_data.history acc and loss data over the epochs (train and validation)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_acc = historical_data.history['acc']\n",
    "train_loss = historical_data.history['loss']\n",
    "\n",
    "val_acc = historical_data.history['val_acc'] # validation\n",
    "val_loss = historical_data.history['val_loss'] # validation\n",
    "\n",
    "range_epochs = range(n_epochs)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 6))\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.plot(range_epochs, train_acc, 'bo', label='Training acc')\n",
    "plt.plot(range_epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# plt.figure()\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.plot(range_epochs, train_loss, 'bo', label='Training loss')\n",
    "plt.plot(range_epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(os.path.join(PATH_TO_DF, \"VGG19_bn_acc_loss.png\"), bbox_inches='tight') # png 70kb vs jpg 135 kb\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras import optimizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the model weights files.\n",
    "# weights_path = '../keras/examples/vgg16_weights.h5'\n",
    "top_model_weights_path = '../data/output_convnet/bn_VGG16_model.h5'\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'train/'\n",
    "validation_data_dir = 'validation/'\n",
    "\n",
    "nb_train_samples = 3000\n",
    "nb_validation_samples = 1000\n",
    "epochs = 50\n",
    "batch_size = 5 # con 5 funciona con 16 es imposible probar con 10\n",
    "num_classes = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the VGG16 network\n",
    "input_tensor = Input(shape=(img_width,img_height,3))\n",
    "base_model = applications.VGG19(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.output_shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "hdf5_file = h5py.File(top_model_weights_path, mode='r')\n",
    "# print(len(hdf5_file))\n",
    "print(list(hdf5_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "    print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(top_model.layers):\n",
    "    print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model.load_weights(\"../data/output_convnet/bn_VGG19_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_total = Model(input= base_model.input, output= top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "    print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(top_model.layers):\n",
    "    print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model_total.layers):\n",
    "    print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model_total.layers):\n",
    "    if layer.trainable:\n",
    "        print(\"layer {0:d}, {1:s} is trainable\".format(i, layer.name))\n",
    "    else:\n",
    "        print(\"layer {0:d}, {1:s} is freezed\".format(i, layer.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model_total.layers[:17]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model_total.layers):\n",
    "    if layer.trainable:\n",
    "        print(\"layer {0:d}, {1:s} is trainable\".format(i, layer.name))\n",
    "    else:\n",
    "        print(\"layer {0:d}, {1:s} is freezed\".format(i, layer.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_total.compile(optimizer=SGD(lr=1e-4, momentum=0.9),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# fine-tune the model\n",
    "\n",
    "# añadir medidas de acc loss como el bottleneck\n",
    "\n",
    "ti_ftuning = time.time()\n",
    "\n",
    "\n",
    "historical_data = model_total.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=nb_train_samples,\n",
    "    epochs=epochs,\n",
    "    verbose = 1,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples)\n",
    "\n",
    "tf_ftuning = time.time()\n",
    "tt_ftuning = tf_ftuning - ti_ftuning\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(tt_ftuning)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "PATH_TO_DF = \"../data/output_convnet\"\n",
    "\n",
    "acc = historical_data.history['acc']\n",
    "val_acc = historical_data.history['val_acc']\n",
    "loss = historical_data.history['loss']\n",
    "val_loss = historical_data.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_total.save_weights('../data/output_convnet/ft_VGG19_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "hdf5_file = h5py.File('../data/output_convnet/ft_VGG19_model.h5', mode='r')\n",
    "print(list(hdf5_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the VGG16 network\n",
    "# input_tensor = Input(shape=(img_width,img_height,3))\n",
    "model_base_pred = applications.VGG19(weights='imagenet', include_top=False, input_tensor=input_tensor)  \n",
    "print(\"model base for predition loaded\")\n",
    "\n",
    "# build top model  \n",
    "model_top_pred = Sequential()  \n",
    "model_top_pred.add(Flatten(input_shape=model_base_pred.output_shape[1:]))  \n",
    "model_top_pred.add(Dense(256, activation='relu'))  \n",
    "model_top_pred.add(Dropout(0.5))  \n",
    "model_top_pred.add(Dense(8, activation='softmax'))\n",
    "print()\n",
    "print(\"model top for predition loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_total_pred = Model(input= model_base_pred.input, output= model_top_pred(model_base_pred.output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(model_total_pred.layers):\n",
    "    print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los pesos anteriormente obtenidos en el entrenamiento\n",
    "model_total_pred.load_weights(\"../data/output_convnet/ft_VGG19_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = 'test/'\n",
    "batch_size_test = 5 # probamos con esto\n",
    "\n",
    "test_convnet = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "test_generator = test_convnet.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle =False,\n",
    "    class_mode='categorical')\n",
    "\n",
    "cat_dict = test_generator.class_indices\n",
    "\n",
    "inverse_coding = {v: k for k, v in cat_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "test_data_dir = \"test/\"\n",
    "\n",
    "# listado del train\n",
    "img_test_real = [] # listado de las imagenes pareado con img_cat_real\n",
    "img_cat_real = [] # categorias de las imagenes\n",
    "\n",
    "\n",
    "img_cat_pred = [] # elementos predichos por la convnet\n",
    "\n",
    "img_folder = sorted([folder for folder in os.listdir(test_data_dir)\n",
    "                  if os.path.isdir(os.path.join(test_data_dir, folder))])\n",
    "\n",
    "for index_folder, category in enumerate(img_folder):\n",
    "    \n",
    "    folder = os.path.join(test_data_dir, category)\n",
    "\n",
    "    for index_img, img in enumerate(os.listdir(folder)):\n",
    "        \n",
    "        if img.endswith(\".tif\"): # just in case there are other kind of files like .db\n",
    "            img_test_real.append(os.path.join(folder, img))\n",
    "            img_cat_real.append(img_folder[index_folder])\n",
    "\n",
    "    print(\"Category {0:s} has {1:d} images.\".format(category, index_img+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, img_path in enumerate(img_test_real):\n",
    "    \n",
    "    if not index % 100:\n",
    "        print(\"image {0:d} processed\".format(index))\n",
    "        \n",
    "    # pre process\n",
    "    img = image.load_img(img_path, target_size=(150, 150))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    \n",
    "    x = x / 255\n",
    "    \n",
    "    # prediction\n",
    "    x = model_total_pred.predict(x)\n",
    "    \n",
    "    # label\n",
    "    label = inverse_coding[np.argmax(x)]\n",
    "    \n",
    "    # store the label\n",
    "    img_cat_pred.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cat_pred_arr = np.array(img_cat_pred)\n",
    "img_cat_real_arr = np.array(img_cat_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "ft_conf_mat = confusion_matrix(img_cat_real_arr,img_cat_pred_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',size=20)\n",
    "    plt.xlabel('Predicted label',size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_IMG = \"../data/input_dataset\"\n",
    "class_names = sorted([folder for folder in os.listdir(PATH_TO_IMG)\n",
    "                      if os.path.isdir(os.path.join(PATH_TO_IMG, folder))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 6))\n",
    "\n",
    "fig.add_subplot(1,2,1)\n",
    "plot_confusion_matrix(ft_conf_mat, classes=class_names, title='Confusion matrix')\n",
    "\n",
    "fig.add_subplot(1,2,2)\n",
    "plot_confusion_matrix(ft_conf_mat, classes=class_names, normalize=True, title='Confusion matrix')\n",
    "\n",
    "plt.savefig(os.path.join(PATH_TO_DF, \"ft_confmat_VGG19.png\"), bbox_inches='tight') # png 70kb vs jpg 135 kb\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
