{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red convolucional - Inception\n",
    "\n",
    "1. Introducción arquitectura VGG_16.\n",
    "2. Provision de imágenes a la arquitectura. Pre procesado de imágenes.\n",
    "    * Estructura listado\n",
    "3. Implementación arquitectura.\n",
    "    * Keras\n",
    "    * Capas\n",
    "    * Arquitectura pre entrenada - Imagenet \n",
    "4. Transferencia de aprendizaje: descriptores profundos.\n",
    "    * Reducción dimensionalidad\n",
    "    * Clasificación\n",
    "    * Medidas de desempeño\n",
    "4. Transferencia de aprendizaje: Fine tuning.\n",
    "    * Medidas de desempeño\n",
    "5. Mejoras\n",
    "    * Provisión de imágenes: hd5py - TFrecords\n",
    "    * Implementación arquitectura: Tensorflow\n",
    "6. Resultados - Tablas. Resumen\n",
    "7. Discusión resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción arquitectura VGG_16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-16.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provision de imágenes a la arquitectura. Pre procesado de imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_40 (ZeroPaddi (None, 226, 226, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_41 (ZeroPaddi (None, 226, 226, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_42 (ZeroPaddi (None, 114, 114, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_43 (ZeroPaddi (None, 114, 114, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_44 (ZeroPaddi (None, 58, 58, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_45 (ZeroPaddi (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_46 (ZeroPaddi (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_47 (ZeroPaddi (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_48 (ZeroPaddi (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_49 (ZeroPaddi (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_50 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_50 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_51 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_52 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224,3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#top layer of the VGG net\n",
    "model.add(Dense(4096, activation=None))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation=None))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/fchollet/deep-learning-models/releases\n",
    "# format h5py\n",
    "model.load_weights(\"pretrained-model/Keras-VGG16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD()\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 82s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# check this cell!!!!\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# prebuild model with pre-trained weights on imagenet\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Extracción descriptores profundos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-built and pre-trained deep learning VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=True)\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo con una fotografía deep features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need pre processing function in another ipython\n",
    "# %run '../dataset/dataset-v0.1-spa.ipynb' # improve this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is on dataset-v0.1-spa.ipynb\n",
    "\n",
    "# 1.- get the imgList structure\n",
    "import os\n",
    "import re\n",
    "\n",
    "PATH_TO_IMG = \"../img-raw\"\n",
    "\n",
    "imgList = []\n",
    "\n",
    "for imgCategory in imgCategories:\n",
    "    imgFolder = os.path.join(PATH_TO_IMG, imgCategory)\n",
    "    \n",
    "    for img in os.listdir(imgFolder):\n",
    "        if img.endswith(\".tif\"): # just in case there are other kind of files like .db\n",
    "            \n",
    "            # windows users: backslash management method replace is used [it is not mandatory tbh]\n",
    "            #imgList.append(os.path.join(imgFolder, img).replace(\"\\\\\",\"/\"))\n",
    "            \n",
    "            # linux users:\n",
    "            imgList.append(os.path.join(imgFolder, img))\n",
    "\n",
    "\n",
    "# 2.- pre process the image\n",
    "from PIL import Image # imported before\n",
    "\n",
    "# http://effbot.org/imagingbook/introduction.htm\n",
    "\n",
    "def img_preprocessing(arch, img, width = 150, height = 150):\n",
    "    \n",
    "    imgAux = Image.open(img)\n",
    "    if(arch==\"VGG16\"):\n",
    "        return imgAux.resize((224,224))\n",
    "\n",
    "    #add new archs\n",
    "    \n",
    "    return img\n",
    "\n",
    "imgExample = img_preprocessing(\"VGG16\", imgList[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fc1 vs fc2: la 1 es una capa en la que sus pesos no estan tan ajustados al cojunto de dato como la segunda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jon/anaconda3/envs/test/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"fc...)`\n",
      "  \n",
      "/home/jon/anaconda3/envs/test/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"fc...)`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer fc1:  (1, 4096)\n",
      "Layer fc2:  (1, 4096)\n",
      "Layer fc1:  4096\n"
     ]
    }
   ],
   "source": [
    "# fc1\n",
    "model = Model(input=base_model.input, output=base_model.get_layer('fc1').output)\n",
    "\n",
    "dfImgExample_fc1  = image.img_to_array(imgExample)\n",
    "dfImgExample_fc1  = np.expand_dims(dfImgExample_fc1 , axis=0)\n",
    "dfImgExample_fc1  = preprocess_input(dfImgExample_fc1 )\n",
    "\n",
    "# get the features from this block\n",
    "features_fc1  = model.predict(dfImgExample_fc1) # numpy.ndarray\n",
    "\n",
    "# fc2\n",
    "model = Model(input=base_model.input, output=base_model.get_layer('fc1').output)\n",
    "\n",
    "dfImgExample_fc2 = image.img_to_array(imgExample)\n",
    "dfImgExample_fc2 = np.expand_dims(dfImgExample_fc2, axis=0)\n",
    "dfImgExample_fc2 = preprocess_input(dfImgExample_fc2)\n",
    "\n",
    "# get the features from this block\n",
    "features_fc2 = model.predict(dfImgExample_fc2)\n",
    "\n",
    "\n",
    "print(\"Layer fc1: \",features_fc1.shape)\n",
    "print(\"Layer fc2: \",features_fc2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature map 2x4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jon/anaconda3/envs/test/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"fc...)`\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# example with 2 images\n",
    "exampleList = []\n",
    "exampleList.append(imgList[0])\n",
    "exampleList.append(imgList[1])\n",
    "\n",
    "# run the whole imgList - extraction of fc1 layer\n",
    "from PIL import Image # imported before\n",
    "\n",
    "# structure \n",
    "ndeepfeatures = 4096 # get the shape from the model instead\n",
    "dfmap = np.empty((len(exampleList),ndeepfeatures)) # just 2 images\n",
    "imgLabels = [] # check if it is better in another place\n",
    "\n",
    "# imgList deep feature map\n",
    "for i, img in enumerate(exampleList):\n",
    "    model = Model(input=base_model.input, output=base_model.get_layer('fc1').output)\n",
    "    \n",
    "    # pre procesado\n",
    "    aux = img_preprocessing(\"VGG16\", img)\n",
    "    \n",
    "    aux = image.img_to_array(aux)\n",
    "    aux = np.expand_dims(aux, axis=0)\n",
    "    aux = preprocess_input(aux)\n",
    "\n",
    "    # get the features from this block\n",
    "    aux  = model.predict(aux)\n",
    "    \n",
    "    # fill the structure\n",
    "    dfmap[i,:] = np.squeeze(aux, axis=0)\n",
    "    imgLabels.append(re.split('_\\d+',img.split('/')[2])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se guardarán los resultados en unos ficheros para no tener que volver a repetir este proceso que necesita mucho tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(2, 4096)\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# check the dimensions of the structures created\n",
    "print(type(dfmap))\n",
    "print(dfmap.shape)\n",
    "print(type(imgLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(dfmap, open('feature-maps/example_dfmap', 'wb'))\n",
    "pickle.dump(imgLabels, open('feature-maps/example_imgLabels', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Procesamiento de todo el conjunto de imágenes - deep features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jon/anaconda3/envs/test/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"fc...)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image nº %s: 1000\n",
      "Processing image nº %s: 2000\n",
      "Processing image nº %s: 3000\n",
      "Processing image nº %s: 4000\n"
     ]
    }
   ],
   "source": [
    "# run the whole imgList - extraction of fc1 layer\n",
    "from PIL import Image # imported before\n",
    "\n",
    "# structure \n",
    "ndeepfeatures = 4096 # get the shape from the model instead\n",
    "dfmap = np.empty((len(imgList),ndeepfeatures))\n",
    "imgLabels = [] # check if it is better in another place\n",
    "\n",
    "# imgList deep feature map\n",
    "for i, img in enumerate(imgList):\n",
    "    if (i == 1000) or (i == 2000) or (i == 3000) or (i == 4000):\n",
    "        print(\"Processing image nº %s:\" % i)\n",
    "        \n",
    "    model = Model(input=base_model.input, output=base_model.get_layer('fc1').output)\n",
    "    \n",
    "    # pre procesado\n",
    "    aux = img_preprocessing(\"VGG16\", img)\n",
    "    \n",
    "    aux = image.img_to_array(aux)\n",
    "    aux = np.expand_dims(aux, axis=0)\n",
    "    aux = preprocess_input(aux)\n",
    "\n",
    "    # get the features from this block\n",
    "    aux  = model.predict(aux)\n",
    "    \n",
    "    # fill the structure\n",
    "    dfmap[i,:] = np.squeeze(aux, axis=0)\n",
    "    imgLabels.append(re.split('_\\d+',img.split('/')[2])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Guardamos los resultados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hd5py (revisar)\n",
    "pickle.dump(dfmap, open('feature-maps/dfmap', 'wb'))\n",
    "pickle.dump(imgLabels, open('feature-maps/imgLabels', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM - sin reduccion de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrenamiento de las últimas capas Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "# prebuild model with pre-trained weights on imagenet\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "\n",
    "\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# create the base pre-trained model. Exclude the top !!\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False) \n",
    "\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
