{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red convolucional - Inception\n",
    "\n",
    "1. Introducción arquitectura VGG_16.\n",
    "2. Provision de imágenes a la arquitectura. Pre procesado de imágenes.\n",
    "    * Estructura listado\n",
    "3. Implementación arquitectura.\n",
    "    * Keras\n",
    "    * Capas\n",
    "    * Arquitectura pre entrenada - Imagenet \n",
    "4. Transferencia de aprendizaje: descriptores profundos.\n",
    "    * Reducción dimensionalidad\n",
    "    * Clasificación\n",
    "    * Medidas de desempeño\n",
    "4. Transferencia de aprendizaje: Fine tuning.\n",
    "    * Medidas de desempeño\n",
    "5. Mejoras\n",
    "    * Provisión de imágenes: hd5py - TFrecords\n",
    "    * Implementación arquitectura: Tensorflow\n",
    "6. Resultados - Tablas. Resumen\n",
    "7. Discusión resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción arquitectura VGG_16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-16.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provision de imágenes a la arquitectura. Pre procesado de imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conda install -c conda-forge keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d_1 (ZeroPaddin (None, 226, 226, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 226, 226, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 114, 114, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 114, 114, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 58, 58, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#https://blog.keras.io/img/imgclf/vgg16_original.png\n",
    "#fuente https://blog.keras.io/\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224,3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#top layer of the VGG net\n",
    "model.add(Dense(4096, activation=None))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation=None))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/fchollet/deep-learning-models/releases\n",
    "# format h5py\n",
    "model.load_weights(\"pretrained-model/Keras-VGG16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = SGD()\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check this cell!!!!\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# prebuild model with pre-trained weights on imagenet\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Extracción descriptores profundos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_8 (None, 224, 224, 3)\n",
      "1 block1_conv1 (None, 224, 224, 64)\n",
      "2 block1_conv2 (None, 224, 224, 64)\n",
      "3 block1_pool (None, 112, 112, 64)\n",
      "4 block2_conv1 (None, 112, 112, 128)\n",
      "5 block2_conv2 (None, 112, 112, 128)\n",
      "6 block2_pool (None, 56, 56, 128)\n",
      "7 block3_conv1 (None, 56, 56, 256)\n",
      "8 block3_conv2 (None, 56, 56, 256)\n",
      "9 block3_conv3 (None, 56, 56, 256)\n",
      "10 block3_pool (None, 28, 28, 256)\n",
      "11 block4_conv1 (None, 28, 28, 512)\n",
      "12 block4_conv2 (None, 28, 28, 512)\n",
      "13 block4_conv3 (None, 28, 28, 512)\n",
      "14 block4_pool (None, 14, 14, 512)\n",
      "15 block5_conv1 (None, 14, 14, 512)\n",
      "16 block5_conv2 (None, 14, 14, 512)\n",
      "17 block5_conv3 (None, 14, 14, 512)\n",
      "18 block5_pool (None, 7, 7, 512)\n",
      "19 flatten (None, 25088)\n",
      "20 fc1 (None, 4096)\n",
      "21 fc2 (None, 4096)\n",
      "22 predictions (None, 1000)\n"
     ]
    }
   ],
   "source": [
    "# pre-built and pre-trained deep learning VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=True)\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo con una fotografía deep features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need pre processing function in another ipython\n",
    "# %run '../dataset/dataset-v0.1-spa.ipynb' # improve this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this code is on dataset-v0.1-spa.ipynb\n",
    "\n",
    "# 1.- get the imgList structure\n",
    "import os\n",
    "import re\n",
    "\n",
    "PATH_TO_IMG = \"../img-raw\"\n",
    "\n",
    "imgList = []\n",
    "\n",
    "for imgCategory in imgCategories:\n",
    "    imgFolder = os.path.join(PATH_TO_IMG, imgCategory)\n",
    "    \n",
    "    for img in os.listdir(imgFolder):\n",
    "        if img.endswith(\".tif\"): # just in case there are other kind of files like .db\n",
    "            \n",
    "            # windows users: backslash management method replace is used [it is not mandatory tbh]\n",
    "            #imgList.append(os.path.join(imgFolder, img).replace(\"\\\\\",\"/\"))\n",
    "            \n",
    "            # linux users:\n",
    "            imgList.append(os.path.join(imgFolder, img))\n",
    "\n",
    "\n",
    "# 2.- pre process the image\n",
    "from PIL import Image # imported before\n",
    "\n",
    "# http://effbot.org/imagingbook/introduction.htm\n",
    "\n",
    "def img_preprocessing(arch, img, width = 150, height = 150):\n",
    "    \n",
    "    imgAux = Image.open(img)\n",
    "    if(arch==\"VGG16\"):\n",
    "        return imgAux.resize((224,224))\n",
    "\n",
    "    #add new archs\n",
    "    \n",
    "    return img\n",
    "\n",
    "imgExample = img_preprocessing(\"VGG16\", imgList[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fc1 vs fc2: la 1 es una capa en la que sus pesos no estan tan ajustados al cojunto de dato como la segunda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fc1\n",
    "model = Model(input=base_model.input, output=base_model.get_layer('fc1').output)\n",
    "\n",
    "dfImgExample_fc1  = image.img_to_array(imgExample)\n",
    "dfImgExample_fc1  = np.expand_dims(dfImgExample_fc1 , axis=0)\n",
    "dfImgExample_fc1  = preprocess_input(dfImgExample_fc1 )\n",
    "\n",
    "# get the features from this block\n",
    "features_fc1  = model.predict(dfImgExample_fc1) # numpy.ndarray\n",
    "\n",
    "# fc2\n",
    "model = Model(input=base_model.input, output=base_model.get_layer('fc2').output)\n",
    "\n",
    "dfImgExample_fc2 = image.img_to_array(imgExample)\n",
    "dfImgExample_fc2 = np.expand_dims(dfImgExample_fc2, axis=0)\n",
    "dfImgExample_fc2 = preprocess_input(dfImgExample_fc2)\n",
    "\n",
    "# get the features from this block\n",
    "features_fc2 = model.predict(dfImgExample_fc2)\n",
    "\n",
    "\n",
    "print(\"Layer fc1: \",features_fc1.shape)\n",
    "print(\"Layer fc2: \",features_fc2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature map 2x4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example with 2 images\n",
    "exampleList = []\n",
    "exampleList.append(imgList[0])\n",
    "exampleList.append(imgList[1])\n",
    "\n",
    "# run the whole imgList - extraction of fc1 layer\n",
    "from PIL import Image # imported before\n",
    "\n",
    "# structure \n",
    "ndeepfeatures = 4096 # get the shape from the model instead\n",
    "dfmap = np.empty((len(exampleList),ndeepfeatures)) # just 2 images\n",
    "imgLabels = [] # check if it is better in another place\n",
    "\n",
    "# imgList deep feature map\n",
    "for i, img in enumerate(exampleList):\n",
    "    model = Model(input=base_model.input, output=base_model.get_layer('fc1').output)\n",
    "    \n",
    "    # pre procesado\n",
    "    aux = img_preprocessing(\"VGG16\", img)\n",
    "    \n",
    "    aux = image.img_to_array(aux)\n",
    "    aux = np.expand_dims(aux, axis=0)\n",
    "    aux = preprocess_input(aux)\n",
    "\n",
    "    # get the features from this block\n",
    "    aux  = model.predict(aux)\n",
    "    \n",
    "    # fill the structure\n",
    "    dfmap[i,:] = np.squeeze(aux, axis=0)\n",
    "    imgLabels.append(re.split('_\\d+',img.split('/')[2])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se guardarán los resultados en unos ficheros para no tener que volver a repetir este proceso que necesita mucho tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check the dimensions of the structures created\n",
    "print(type(dfmap))\n",
    "print(dfmap.shape)\n",
    "print(type(imgLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(dfmap, open('feature-maps/VGG16-example_dfmap', 'wb'))\n",
    "pickle.dump(imgLabels, open('feature-maps/VGG16-example_imgLabels', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Procesamiento de todo el conjunto de imágenes - deep features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run the whole imgList - extraction of fc1 layer\n",
    "from PIL import Image # imported before\n",
    "\n",
    "# structure \n",
    "ndeepfeatures = 4096 # get the shape from the model instead\n",
    "dfmap = np.empty((len(imgList),ndeepfeatures))\n",
    "imgLabels = [] # check if it is better in another place\n",
    "\n",
    "# imgList deep feature map\n",
    "for i, img in enumerate(imgList):\n",
    "    if (i == 1000) or (i == 2000) or (i == 3000) or (i == 4000):\n",
    "        print(\"Processing image nº %s:\" % i)\n",
    "        \n",
    "    model = Model(input=base_model.input, output=base_model.get_layer('fc1').output)\n",
    "    \n",
    "    # pre procesado\n",
    "    aux = img_preprocessing(\"VGG16\", img)\n",
    "    \n",
    "    aux = image.img_to_array(aux)\n",
    "    aux = np.expand_dims(aux, axis=0)\n",
    "    aux = preprocess_input(aux)\n",
    "\n",
    "    # get the features from this block\n",
    "    aux  = model.predict(aux)\n",
    "    \n",
    "    # fill the structure\n",
    "    dfmap[i,:] = np.squeeze(aux, axis=0)\n",
    "    imgLabels.append(re.split('_\\d+',img.split('/')[2])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Guardamos los resultados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hd5py (revisar)\n",
    "pickle.dump(dfmap, open('feature-maps/VGG16-dfmap', 'wb'))\n",
    "pickle.dump(imgLabels, open('feature-maps/VGG16-imgLabels', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM - sin reduccion de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using the bottleneck features of a pre-trained network\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Entrenamiento de las últimas capas Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transformacion de imgList a dataframe ¿?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://blog.keras.io/img/imgclf/vgg16_modified.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# check the imports\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: input_15, trainable - False\n",
      "Layer output:  (None, None, None, 3)\n",
      "Layer 1: block1_conv1, trainable - False\n",
      "Layer output:  (None, None, None, 64)\n",
      "Layer 2: block1_conv2, trainable - False\n",
      "Layer output:  (None, None, None, 64)\n",
      "Layer 3: block1_pool, trainable - False\n",
      "Layer output:  (None, None, None, 64)\n",
      "Layer 4: block2_conv1, trainable - False\n",
      "Layer output:  (None, None, None, 128)\n",
      "Layer 5: block2_conv2, trainable - False\n",
      "Layer output:  (None, None, None, 128)\n",
      "Layer 6: block2_pool, trainable - False\n",
      "Layer output:  (None, None, None, 128)\n",
      "Layer 7: block3_conv1, trainable - False\n",
      "Layer output:  (None, None, None, 256)\n",
      "Layer 8: block3_conv2, trainable - False\n",
      "Layer output:  (None, None, None, 256)\n",
      "Layer 9: block3_conv3, trainable - False\n",
      "Layer output:  (None, None, None, 256)\n",
      "Layer 10: block3_pool, trainable - False\n",
      "Layer output:  (None, None, None, 256)\n",
      "Layer 11: block4_conv1, trainable - False\n",
      "Layer output:  (None, None, None, 512)\n",
      "Layer 12: block4_conv2, trainable - False\n",
      "Layer output:  (None, None, None, 512)\n",
      "Layer 13: block4_conv3, trainable - False\n",
      "Layer output:  (None, None, None, 512)\n",
      "Layer 14: block4_pool, trainable - False\n",
      "Layer output:  (None, None, None, 512)\n",
      "Layer 15: block5_conv1, trainable - False\n",
      "Layer output:  (None, None, None, 512)\n",
      "Layer 16: block5_conv2, trainable - False\n",
      "Layer output:  (None, None, None, 512)\n",
      "Layer 17: block5_conv3, trainable - False\n",
      "Layer output:  (None, None, None, 512)\n",
      "Layer 18: block5_pool, trainable - False\n",
      "Layer output:  (None, None, None, 512)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# bottle neck --> es como la 1 parte\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "# create the base pre-trained model. Exclude the top !!\n",
    "base_model_bn = VGG16(weights='imagenet', include_top=False)\n",
    "# all layers will be non-trainable (weights will not be updated)\n",
    "for layer in base_model_bn.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for i, layer in enumerate(base_model_bn.layers):\n",
    "    print(\"Layer %d: %s, trainable - %s\" % (i, layer.name, layer.trainable))\n",
    "    print(\"Layer output: \",layer.output_shape)\n",
    "\n",
    "print(base_model_bn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-dfe2798334e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdataSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovida\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mdataSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mimgLabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimgList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "# train and validation sets from imgList y labels\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "imgList = []\n",
    "\n",
    "for imgCategory in imgCategories:\n",
    "    imgFolder = os.path.join(PATH_TO_IMG, imgCategory)\n",
    "    \n",
    "    for img in os.listdir(imgFolder):\n",
    "        if img.endswith(\".tif\"): # just in case there are other kind of files like .db\n",
    "            \n",
    "            # windows users: backslash management method replace is used [it is not mandatory tbh]\n",
    "            #imgList.append(os.path.join(imgFolder, img).replace(\"\\\\\",\"/\"))\n",
    "            \n",
    "            # linux users:\n",
    "            imgList.append(os.path.join(imgFolder, img))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "imgLabels = []\n",
    "\n",
    "for img in imgList:\n",
    "    imgLabels.append(re.split('_\\d+',img.split('/')[2])[0])\n",
    "\n",
    "dataSet = []\n",
    "for i, movida in enumerate(imgLabels):\n",
    "    dataSet[i-1] =imgLabels[i-1] + imgList[i-1]\n",
    "\n",
    "\n",
    "# train_set_bn\n",
    "# validation_set_bn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# better with pickle?\n",
    "\n",
    "# prediction with train and validation sets\n",
    "\n",
    "train_features_bn = base_model_bn.predict(train_set_bn)\n",
    "np.save(open('train_features_bn.npy', 'w'), train_features_bn)\n",
    "\n",
    "validation_features_bn = base_model_bn.predict(validation_set_bn)\n",
    "np.save(open('validation_features_bn.npy', 'w'), validation_features_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the data saved in npys mandatory¿?\n",
    "# de train_features hay que extraer train_data y train_labels\n",
    "# de validation_features hay que estraer validation_data y validation_labels\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_model_bn = Sequential()\n",
    "top_model_bn.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "top_model_bn.add(Dense(4096, activation='relu'))\n",
    "top_model_bn.add(Dropout(0.5))\n",
    "top_model_bn.add(Dense(4096, activation='relu'))\n",
    "top_model_bn.add(Dropout(0.5))\n",
    "top_model_bn.add(Dense(8, activation='softmax'))\n",
    "\n",
    "# en la 1 parte\n",
    "# sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "\n",
    "\n",
    "#check the args\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#\n",
    "bn_result = model.fit(train_data, train_labels, epochs=epochs,\\\\\n",
    "                      batch_size=batch_size, validation_data=(validation_data, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# comparar la estructura que tiene los labels y son los reales con los predichos por el modelo en bn_result\n",
    "\n",
    "# matriz confusion 8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# segunda parte fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# repasar con cuidad la forma de cargar y contruir el modelo sobre todo los pesos\n",
    "\n",
    "'''\n",
    "igual me tienes que explicar la diferencia entre usar los pesos de imagenet vs estos pesos\n",
    "\n",
    "yo tengo estos pesos\n",
    "\n",
    "vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
    "vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "\n",
    "\n",
    "chollet hace esto\n",
    "\n",
    "link original https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "en github https://gist.github.com/fchollet/7eb39b44eb9e16e59632d25fb3119975 linea 69\n",
    "\n",
    "\n",
    "carga los pesos de top_model sin haber unido esa capa con las convolucionales vgg16 no_top = true\n",
    "la variable de esos pesos es top_model_weights_path = 'fc_model.h5' a que se corresponde este fichero\n",
    "una vez cargados si que une esa capa con el resto de la convnet para luego poner la base de la convnet\n",
    "como no entrenable para fijar los pesos y entrenar la capa top\n",
    "\n",
    "por lo que tenemos base con los pesos de imagenet no entrenable + top con esos pesos desconocidos entrenable\n",
    "\n",
    "una vez hecho esto compila el modelo con\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "'''\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/fchollet/f35fbc80e066a49d65f1688a7e99f069\n",
    "\n",
    "# PUEDE SER QUE SEA LA MISMA EN AMBAS APROXIMACIONES BOTTLE NECK Y FINE TUNING\n",
    "# add the top layer (classifier) to the conv-net\n",
    "# top_model_bn = Sequential()\n",
    "# top_model_bn.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "# top_model_bn.add(Dense(4096, activation='relu'))\n",
    "# top_model_bn.add(Dropout(0.5))\n",
    "# top_model_bn.add(Dense(4096, activation='relu'))\n",
    "# top_model_bn.add(Dropout(0.5))\n",
    "# top_model_bn.add(Dense(8, activation='softmax'))\n",
    "\n",
    "# add the last layer to the bottle neck model\n",
    "# https://github.com/flyyufelix/cnn_finetune/blob/master/vgg16.py\n",
    "\n",
    "\n",
    "\n",
    "# https://gist.github.com/fchollet/7eb39b44eb9e16e59632d25fb3119975\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "# top_model_bn.load_weights(top_model_weights_path)\n",
    "\n",
    "# # add the model on top of the convolutional base\n",
    "# model_bn.add(top_model_bn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # all layers will be non-trainable (weights will not be updated)\n",
    "# for layer in base_model_bn.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# for i, layer in enumerate(base_model_bn.layers):\n",
    "#     print(\"Layer %d: %s, trainable - %s\" % (i, layer.name, layer.trainable))\n",
    "#     print(\"Layer output: \",layer.output_shape)\n",
    "\n",
    "# print(base_model_bn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: input_13, trainable - False\n",
      "Layer output:  (None, None, None, 3)\n",
      "Layer 1: block1_conv1, trainable - False\n",
      "Layer output:  (None, None, None, 64)\n",
      "Layer 2: block1_conv2, trainable - False\n",
      "Layer output:  (None, None, None, 64)\n",
      "Layer 3: block1_pool, trainable - False\n",
      "Layer output:  (None, None, None, 64)\n",
      "Layer 4: block2_conv1, trainable - False\n",
      "Layer output:  (None, None, None, 128)\n",
      "Layer 5: block2_conv2, trainable - False\n",
      "Layer output:  (None, None, None, 128)\n",
      "Layer 6: block2_pool, trainable - False\n",
      "Layer output:  (None, None, None, 128)\n",
      "Layer 7: block3_conv1, trainable - False\n",
      "Layer output:  (None, None, None, 256)\n",
      "Layer 8: block3_conv2, trainable - False\n",
      "Layer output:  (None, None, None, 256)\n",
      "Layer 9: block3_conv3, trainable - False\n",
      "Layer output:  (None, None, None, 256)\n",
      "Layer 10: block3_pool, trainable - False\n",
      "Layer output:  (None, None, None, 256)\n",
      "Layer 11: block4_conv1, trainable - False\n",
      "Layer output:  (None, None, None, 512)\n",
      "Layer 12: block4_conv2, trainable - False\n",
      "Layer output:  (None, None, None, 512)\n",
      "Layer 13: block4_conv3, trainable - False\n",
      "Layer output:  (None, None, None, 512)\n",
      "Layer 14: block4_pool, trainable - False\n",
      "Layer output:  (None, None, None, 512)\n",
      "Layer 15: block5_conv1, trainable - True\n",
      "Layer output:  (None, None, None, 512)\n",
      "Layer 16: block5_conv2, trainable - True\n",
      "Layer output:  (None, None, None, 512)\n",
      "Layer 17: block5_conv3, trainable - True\n",
      "Layer output:  (None, None, None, 512)\n",
      "Layer 18: block5_pool, trainable - True\n",
      "Layer output:  (None, None, None, 512)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 7,079,424\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# fine tuning\n",
    "# create the base pre-trained model. Exclude the top !!\n",
    "base_model_ft = VGG16(weights='imagenet', include_top=False) \n",
    "\n",
    "# last conv-layer will be trainable (weights will not be updated)\n",
    "for layer in base_model_ft.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "for i, layer in enumerate(base_model_ft.layers):\n",
    "    print(\"Layer %d: %s, trainable - %s\" % (i, layer.name, layer.trainable))\n",
    "    print(\"Layer output: \",layer.output_shape)\n",
    "\n",
    "print(base_model_ft.summary()) # the architecture is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), \\\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
